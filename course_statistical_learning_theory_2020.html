<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Statistical Learning Theory (2020), Graduate School of Informatics, Kyoto University</title>
<meta http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
<meta content="MSHTML 6.00.6000.17093" name="GENERATOR">
<meta http-equiv="Content-Style-Type" content="text/css"></head>
<body>
<p><font size="+2"><b>Statistical Learning Theory (2020), Graduate School of Informatics, Kyoto University<br>
</b>
</font><br>
Lecturers: <a href="https://hkashima.github.io/index.html">Hisashi Kashima</a> and <a href="hhttps://riken-yamada.github.io/SML2020_yamada.html">Makoto Yamada</a><br>
  Day, time, and room: Monday, 8:45-10:15 / <del>Research bldg. 8, Lecture room 4</del><br>
  This course is held on-line for the time being. Please also check <a href="https://panda.ecs.kyoto-u.ac.jp/portal/site/2020-888-M311-001">the PandA page</a> 
  
<br>
<br>

This course will cover in a broad sense the fundamental theoretical aspects and applicative possibilities of statistical machine learning, which is now a fundamental block of statistical data analysis and data mining. This course will focus first on the supervised and unsupervised learning problems, including a survey of probably approximately correct learning, Bayesian learning as well as other learning theory frameworks. Following this introduction, several
probabilistic models and prediction algorithms, such as the logistic regression, perceptron, and support vector machine will be introduced.
Advanced topic such as online learning, structured prediction, and sparse modeling will be also introduced.

<p></p><strong>[Topics]</strong> (subject to changes of topics)<br>
  1. Statistical Learning Theory<br>
1-1. Introduction to classification & regression: historical perspective, separating hyperplanes and major algorithms<br>
1-2. Probabilistic framework of classification and statistical learning theory: Learning Bounds, Vapnik-Chervonenkis theory<br>
2. Supervised Learning<br>
2-1 Models for Classification: Logistic Regression, Perceptron, Support Vector Machines<br>
2-2 Regularization: Sparse Models (L1 regularization), Bayesian Interpretations<br>
2-3 Model Selection: Performance Measures, Cross-Validation, and Other Information Criterion<br>
3. Advanced topics<br>
3-1 Online learning<br>
3-2 Semi-supervised, Active, and Transfer Learning <br>
<br>
<br>
<strong>[Lecture Slides]</strong> <br>
 1. <a href="SML2020_introduction.pdf">Introduction to machine learning</a><br>
 2. <a href="SML2020_regression.pdf">Regression</a><br>
 3. <a href="SML2020_classification.pdf">Classification</a><br>
 4. <a href="SML2020_learningtheory.pdf">Statistical learning theory</a><br>
 5. <a href="SML2020_evaluation.pdf">Model evaluation and selection</a><br>
 The materials for the latter half of this course are found at <a href="https://riken-yamada.github.io/SML2020_yamada.html">Prof. Yamada's website</a><br>
<br>
<strong>[Homework]</strong><br>
 1. To be announced in the class<br>
<br>
<br>


<strong>[References]</strong><br>
- Lecture slides for the previous years: 
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2014.html">2014</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2015.html">2015</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2016.html">2016</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2017.html">2017</a>, 
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2018.html">2018</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2019.html">2019</a>,<br>
<br>
<br>


</body></html> 
