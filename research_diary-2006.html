<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
<META name="GENERATOR" content="IBM WebSphere Studio Homepage Builder Version 8.0.0.0 for Windows">
<META http-equiv="Content-Style-Type" content="text/css">
<TITLE>機械学習についての日々の研究</TITLE>
</HEAD>
<BODY>
<p><a href="http://www.geocities.jp/kashi_pong/index.html">もどる</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2012.html">2012年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2011.html">2011年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2010.html">2010年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2009.html">2009年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2008.html">2008年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2007.html">2007年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2005.html">2005年の日記</a> 
<a href="http://www.geocities.jp/kashi_pong/research_diary-2004.html">2004年までの日記</a>
</p>
<P><FONT size="+2"><B>（か）研究日記 (2006年)</B></FONT></P>
<HR>
<UL>
  <LI><A name="20061231"></A>2006/12/31 2006年のまとめ
  <UL>
    <LI>2006年も、あまりぱっとする成果は出ませんでした。
    <UL>
      <LI>まあ、ぱっとしてたか、してなかったかは、数年経って分かるものだから、現時点ではなんともいえないんだろけどね。
      
    </UL>
    <LI>2006年は、（引きこもりの自分にしては）多くの（しかもスゴイ）人たちと知りあいになれました。
    <LI>2006年も、ベイジアンにも生成人にもなれませんでした、が、来年も、懲りずに分類ばっかりやりたいと思います。
    <UL>
      <LI>いや、ホントはちょっとくらい生成してみたいかな。
    </UL>
  </UL>
  <LI><A name="20061122"></A>2006/11/22 パーフェクトサンプリングとカーネル
  <UL>
    <LI><A href="http://www.simplex.t.u-tokyo.ac.jp/~kijima/">来嶋さん</A>にパーフェクトサンプリングのお話を再度聞いた。こんどはだいぶ分かった気がする。
    <UL>
      <LI>ちなみに<A href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/book.html">Mackay本</A>に、パーフェクトサンプリングの章がある…持ってたのに知らんかった。
      <LI>基本的には、
      <UL>
        <LI>まず、列挙したい対象のドメインにおいて、列挙したい確率分布について、つりあいを保つようなマルコフ連鎖を設計する。
        <LI>無限回遷移すれば、所望の確率分布に従うサンプルが取り出せるが、無限の過去から遷移を始めて、現在の時点における値をサンプリングするというように考えることにする。
        <LI>ある過去の時点における全ての状態から（同じ乱数を使って）遷移を初めて、今、同じ状態に到達すれば、無限の過去からサンプリングを始めたものが、必ずココに到達するとみなせる。
        <LI>したがって、ｔステップ前からやってみて、同じ状態に到達できなかったら、ｔステップ前からやってみる…っていうように繰り返していって、同じ状態に到達できたら、それが所望のサンプル。
        <LI>ただし、「ある過去の時点における全ての状態」てのは実際には列挙できない。
        <LI>そこで、状態間に半順序をいれる。 また、マルコフ連鎖がこの順序を保存するように設計する。
        <LI>順序の一番小さいものと大きいものが同じ状態に到達すれば、間のやつは全部到達するはずだから、この2つだけ考えればOK。
      </UL>
    </UL>
    <LI>なんだか、研究分野としては、参入しやすい（かもしれない）というイミで、カーネルに近いにおいがする。
    <UL>
      <LI>カーネルの設計の話じたいは、とりたてて学習理論の素養がなくても、カーネル設計の最低限のルールを覚えれば、あとは、いろんな対象に対するカーネル関数を自由に考えてよい。
      適当な表現力をもって、適当な計算時間で計算できるものを一生懸命考えればよい。
      つまり、長い間この分野にいなくても、地頭での思いつきで参入できるという利点がある。
      <LI>パーフェクトサンプリングもなんかそんな雰囲気。 マルコフ連鎖が満たすべきルール（順序を保った遷移を設計するとか）をいくつか覚えて、あとは、いろんな対象を数えるためのマルコフ連鎖を設計すればよい。
      それは、確率過程やらなんやらの素養がなくても、地頭で何か思いつけばいいのだ。
    </UL>
    <LI>カーネルが、何かオシャレなもののカーネルを設計できたらOKだったのと同じように、一生懸命考えて、何かオシャレなものを数えることができたら、それでいいのだ。
    
  </UL>
  <LI><A name="20061018"></A>2006/10/18 協調フィルタリング、リンク予測、マルチタスク学習、アンド、ビヨン
  <UL>
    <LI>これらの問題は、やりたいことは違うが、問題の表現がとても似ている。 というか、同じ。
    <LI>従って、例えば、協調フィルタリングのアプローチをマルチタスク学習に試してみることができる。
    その逆も可。
    <LI>そして、現在別々に提案されているアプローチも統一的に論じることができる（はず）。
    <LI>まず、問題を協調フィルタリングの文脈で書くとすると、データは次のような3つの行列で表現される<BR>
    <IMG src="20061018.jpg" width="182" height="142" border="0">
    <UL>
      <LI>与えられるもの：
      <UL>
        <LI>Yの要素 y<SUB>ij</SUB> は、ユーザー i のアイテム j に対する評価 （ところどころ抜けている）
        <LI>Uの行ベクトルは、それぞれのユーザーの特徴ベクトル。
        <LI>Vの列ベクトルは、それぞれのアイテムの特徴ベクトル。
        <LI>行がユーザーに、列がアイテムに対応する。
      </UL>
      <LI>求めるもの： Yの抜けている部分の値
      <UL>
        <LI>あるいは新しいユーザー（行）が追加されたときの、Yの対応する行ベクトル。
        <LI>あるいは新しいアイテム（列）が追加されたときの、Yの対応する列ベクトル。
      </UL>
      <LI>教師アリ、セミ教師アリ、トランスダクションのような問題として捉えられる。
      <LI>問題のバリエーション： UやVが与えられる場合と、与えられない場合がある。
    </UL>
    <LI>さて、そのほかの問題も同じ形式で書ける。
    <UL>
      <LI>リンク予測（＝ネットワーク構造の予測問題）
      <UL>
        <LI>リンク予測は、ネットワーク構造の観測されている部分から、残りの部分を予測する問題。
        <LI>行や列がネットワークの構成単位（ノード）に相当する。
        <LI>Yは隣接行列を表す。 枝の向きがない場合にはYは対称。
        <LI>UやVはノードの属性を表す。 枝の向きがない場合にはU=V'。
        <LI>タンパク質ネットワークの場合には、Yの左上の方が割と密に分かっていて、あとは分からん、という設定が自然らしい。
        <LI>UやVが与えられない場合もある。 僕が最近取り組んでいたのはこれ。
      </UL>
      <LI>マルチタスク学習
      <UL>
        <LI>複数の関連ある教師アリタスクを（別々に解くのではなく）タスク間の類似性を利用しながら、まとめて解く。
        <LI>行がタスクに、列が事例に相当する。
        <LI>Yは、予測するべき値（分類問題ではクラス）。
        <LI>Vは、データの属性（分類問題の入力）。
        <LI>Uは、通常与えられない。
        <UL>
          <LI>が、タスクの属性があるような場合も当然考えられるだろう。（店舗の属性とか）
        </UL>
      </UL>
    </UL>
    <LI>コンジョイント分析とかも同じような感じで書ける気がするけどよくわからん。
    <LI>ちなみに、先日のNC研究会であった<A href="http://www.ieice.org/ken/program/index.php?layout=&tgs_regid=0d1dac2082b9c8826dbbbc5cdd422c57b62e8c7fe9eaf80198f1796031120057&cmd=show_form&form_code=uAOJ">麻生先生らのサーベイ</A>はとても参考になります。
    <UL>
      <LI>電子的に入手できないのが残念。
    </UL>
  </UL>
  <LI><A name="20061011"></A>2006/10/11 GP、RVM、およびBPM
  <UL>
    <LI><A href="http://www.bic.kyoto-u.ac.jp/pathway/takigawa/">瀧川さん</A>（すごい人）のところからぱくってきた、<A href="http://www.amazon.co.jp/gp/product/026208306X/">HerbrichのLearning Kernel Classifiers</A>本の<A href="http://noplans.org/~1gac/data/bayesian.pdf">3章のまとめ</A>。
    <LI>GP（Gaussian Process）、RVM (Relevance Vector Machine)、BPM（Bayes Point Machine）がまとまっている。
    <LI>まず、基本的には、前者の2つ(GPとRVM)はモデルとしては同じで、線形回帰モデルのパラメータ事前分布が正規分布であるもの。
    <UL>
      <LI>GP（Gaussian Process）は、モデルそのものこと。
      <LI>RVM (Relevance Vector Machine)は、そのパラメータ事前分布を、平均0として、分散を学習するぞ。
      <UL>
        <LI>RVMでほとんどの分散が0になってスパースな予測器ができる。
      </UL>
      <LI>ともに線形回帰がベース（もちろんカーネルによる非線形化が可能）で、最適解がclosed
    formで書ける（行列計算は必要）。
      <LI>シグモイド関数をかませば分類器になる。この場合、最適解は求まらないがLaplace近似や変分法で近似的に解く。
    </UL>
    <LI>一方、BPMは、もともと分類問題からスタート。
    <UL>
      <LI>線形分類器のパラメータは、スケールを変えても同じ分類器になるため、パラメータを単位超級面上の点と解釈できる。
      <LI>で、BPMのパラメータは（この超球面上での）バージョン空間の重心とする。
      <UL>
        <LI>バージョン空間とは、事例と矛盾しないパラメータの集合、ここでは単位超球面上の部分集合
        <LI>BPMのパラメータは、線形分類器のパラメータの事前分布が、バージョン空間の中で一様であるとしたときの期待値に相当する
      </UL>
      <LI>これを求めるのには、当初はMCMCっぽい方法が提案されていたが、現在はもっと簡単なやり方がある。
      <UL>
        <LI>パーセプトロンに食わせる例の順序を変えて、たくさんパーセプトロンを作って、それらのパラメータの平均をとればよい。
      </UL>
    </UL>
  </UL>
  <LI><A name="20061003"></A>2006/10/3 研究のスキル
  <UL>
    <LI><A href="http://research.microsoft.com/Users/simonpj/papers/giving-a-talk/giving-a-talk.htm">「研究のやりかた」のリンク集</A>から２つご紹介
    <UL>
      <LI>マイクロソフトリサーチの人らしい。
      <LI>ちなみに、このページは<A href="http://www.fun.ac.jp/aamas2006/main.html">AAMASでベストペーパ</A>をとった名古屋工大の伊藤先生という方の<A href="http://www.mta.nitech.ac.jp/~ito/">ページ</A>からたどった。
    </UL>
    <LI><A href="http://research.microsoft.com/Users/simonpj/papers/giving-a-talk/writing-a-paper-slides.pdf">「いい論文を書くには」</A>
    <UL>
      <LI>どんなにいいアイデアでも人に言わなければ（そして伝わらなければ）、意味がない。
      論文の目的は、自分の頭から読者の頭にアイデアを伝えることである。
      <LI>研究の進め方は、「アイデア→研究→論文」ではなく「アイデア→論文→研究」だ。
      <UL>
        <LI>書くことで、考えている（考えてない）ことが明らかになる。
        <LI>人に早い段階で伝える → 協力できる可能性が増す。
      </UL>
      <LI>イントロは、問題と貢献を明らかにし、伝える、そんだけにしとけ。
      <UL>
        <LI>&quot;rest of this papers is ...&quot;は使わない、イントロで関連するたびに&quot;(Chapter
        X)&quot;のように書く。
        <UL>
          <LI>なんかこれ新鮮。
        </UL>
        <LI>イントロと内容の間に関連研究の章を入れない。 それは論文の後半に。
        <UL>
          <LI>内容が伝わる前に、これを書いても仕方ない。
        </UL>
      </UL>
      <LI>内容は、記号の定義からはじめない。 まず例を使う 。
      <UL>
        <LI>これをやると、寝ちゃうからって。
        <UL>
          <LI>うん、たしかに寝ちゃう。
        </UL>
        <LI>まず、例をつかってイメージを伝えることを。 イメージを共有してから、記号なりなんなり使う。
      </UL>
      <LI>関連研究には敬意を払う。
      <UL>
        <LI>関連研究に払う敬意をお金のように考えない、それによって論文の価値は減らない。
        <LI>自分の論文の弱みも書く。
        <LI>引用される立場に立ってみて、その立場に立った自分の機嫌を損ねないように書けばイイと。
        たぶん。
      </UL>
      <LI>とにかく論文は早く書き始める。
      <UL>
        <LI>拙速はイカンと。
      </UL>
      <LI>能動態を使え。
      <UL>
        <LI>へえ。
      </UL>
      <LI>あと、もうひとつ心に残ったお言葉、「論文はインプリよりも長生きである。」
      <UL>
        <LI>これからの時代にはわかりませんが…。
      </UL>
      <LI>そんなに「ビジネスにおけるプレゼンテーション」ライクに捉える必要はないかもしれないが、まず、大前提として、すべての読者が自分の論文にとても興味を持って時間を割いて頑張ってありがたく読んでくれるという奢った考えを捨てるということなんだろうと思う。
      読む人がお客様、それも、そこまで自分の話の詳細に興味をもっていない、それも、時間がない人であることを想定して書く。
      
    </UL>
    <LI><A href="http://www.rota.org/hotair/lesson.html">「私が教わっておきたかった１０個のこと」</A>
    <UL>
      <LI>曲解してる気もするけど、心にのこった４つをご紹介。
      <UL>
        <LI>数学の人の話なので、やたら「ヒルベルトでもそうだった」みたいな感じになっています。
      </UL>
      <LI>研究者は（論文よりも）解説記事によって名前が残るもんだ。
      <UL>
        <LI>エライ人でも、研究的業績よりも、教科書の著者として有名とかいう人が多い。
        <UL>
          <LI>たしかにそうかも。有名な教科書の人とかも実はすばらしい業績を残しているのに「あー、なんちゃらブックの人ね。」
        </UL>
        <LI>ここから学ぶべきことは何だろうなあ…とりあえず、ある程度そのトピックを勉強したら、まず、やさしいチュートリアルなり、サーベイなりを書く？
        <UL>
          <LI>たしかに、他の人への貢献度合いという意味では、微妙な拡張で論文かくよりも遥かに高いかあ…。
        </UL>
      </UL>
      <LI>すごいエライ人でも実は２つ３つの技で生きてたりする。
      <UL>
        <LI>いい業績をばんばん出している人も、実はよくよく見てみると、技的には２つ３つの技を繰り返し使っている。
        <LI>要するに、あまり人がもっていない、かつ使える技を、少なくてもいいからきっちり身につけることが大事ですと。
        <UL>
          <LI>ん、なんか安心。 でも実は、その技を見つけるのが大変なんだろうけどね。
        </UL>
      </UL>
      <LI>「ファインマンメソッド」を使え。
      <UL>
        <LI>ときたい問題のストックをもっていて、新しい技に出会ったら、それを解きたい問題に順番に適用してみる。
        <UL>
          <LI>ファインマンは、そういうことをやっていましたよ、とのこと。 でも、実はこれって誰でもやってることかも…。
        </UL>
      </UL>
      <LI>謝辞は惜しみなく。
      <UL>
        <LI>「研究のやりかた」のほうとも重なるけど、とりあえず、人の仕事には敬意を払えと。
      </UL>
    </UL>
  </UL>
  <LI><A name="20060930"></A>2006/9/30 <A href="http://ibisml.org/dmss2006/">DMSS</A>
  <UL>
    <LI>先週、International Workshop on Data-Mining and Statistical Science (DMSS2006)というワークショップに行ってきました。
    <UL>
      <LI>データマイニングの「国際」ワークショップ（参加者はほとんど日本人だが、概ね英語で行われた）
      <LI>ちゃんとホテルの会議室をとって、ちゃんと国際会議っぽい雰囲気、すごい。
      部分的にパラレルセッションになったりして、<A href="http://ibisml.org/dmss2006/program.html">発表</A>は32件、結構な規模である。 招待講演は自分を入れて5名。 参加者は60−70名で、ほぼ大学関係者、データマイニング8割、統計2割という感じ。
    </UL>
    <LI>招待講演のかたがた（一部のスライドは<A href="http://ibisml.org/dmss2006/#invited">ここ</A>に）
    <UL>
      <LI><A href="http://www.kyb.tuebingen.mpg.de/main/staff.php?user=tsuda">津田さん</A>（Max Plank Institute）：グラフマイニングの機械学習への応用
      <UL>
        <LI>問題： 構造データを、部分構造を使った特徴ベクトル表現をしようとすると、次元が高くなりすぎる（＝計算できない）
        <UL>
          <LI>ここでは、グラフのクラスタリングの問題（RNA構造の分類）と、グラフの回帰問題（化合物の活性の予測）などがしたい
        </UL>
        <LI>解決策： 高速なグラフマイニング手法と組み合わせるよ
        <UL>
          <LI>予測に重要な部分構造の選定の部分を、グラフマイニングアルゴリズムをブラックボックスとして呼び出すことで高速化する。
          <LI>目的関数にL1正則化項を入れることで、実際に必要な部分構造の数が少なくなる。
        </UL>
        <LI>マイニングアルゴリズムを作る人は、是非、重み付きで実装してほしいとのこと。
        <UL>
          <LI>（でも、普通の単調減少の評価値じゃなくて、マイナス重み可のやつじゃないとダメだよね。）
        </UL>
      </UL>
      <LI><A href="http://www.lri.fr/~termier/main.php?currentPage=1&lang=us">ターミエさん</A>（統数研）：木のパターンマイニング
      <UL>
        <LI>問題： 物事の間の潜在的なネットワーク構造をとりだしたい
        <UL>
          <LI>マイクロアレイなどの遺伝子同時発現データから遺伝子ネットワーク同定をするという話がある
          <LI>大抵の方法はグリーディな構造探索を使うので、たくさんローカルな解がある。
        </UL>
        <LI>解決策： たくさんの怪しい解のなかから、信頼のおける部分をグラフマイニングで取り出すよ。
        <UL>
          <LI>たくさんの解の中で共通のパターンを見つければそれが信頼のおける部分ネットワーク構造なはず。
          <LI>彼の開発したDAGパターン発見器で見つける。
          <LI><A href="http://www.jaist.ac.jp/jsai2006/program/pdf/100015.pdf">コレ</A>かな。
        </UL>
        <LI>彼は閉集合パターンと呼ばれるパタンを発見する方法の木版「Dryade」の作者
        <LI>歴史的には、先にグラフマイニング（by <A href="http://www.ar.sanken.osaka-u.ac.jp/inokuchiprjp.html">Inokuchi</A>）があって、あとから、木の特性を利用してより高速に掘るという感じで手法が発展していったというのが面白い。
      </UL>
      <LI><A href="http://bonsai.ims.u-tokyo.ac.jp/~yoshidar/">吉田さん</A>（東大・医科研：たぶん<A href="http://www.hgc.jp/~tshibuya/index-j.html">渋谷さん</A>の近く？）：高次元データを扱う方法
      <UL>
        <LI>問題： マイクロアレイデータのように、データ数よりも次元が大きい場合に意味のある結果を出したい
        <UL>
          <LI>たとえばクラスタリングで、次元が大きいと意味のある答えが出ない。
        </UL>
        <LI>解決法： 情報を落とさずに、次元を落とすよ
        <UL>
          <LI>たとえばGaussian Mixtureによるクラスタリングでも、高次元をそのまま扱うのではなく潜在変数を考え、次元を落とした先でのGaussian Mixtureを考える。
        </UL>
        <LI>適切な複雑さのモデルを、よく考えて誠実に設計しているなあ、という印象。
      </UL>
      <LI><A href="http://www.simplex.t.u-tokyo.ac.jp/~kijima/">来嶋さん</A>（東大）： パーフェクトサンプリング
      <UL>
        <LI>問題： MCMC(Markov Chain Monte Carlo)ってどのくらい待てば所望の分布からサンプリングできるの？
        <UL>
          <LI>所望の分布に従ったサンプリングをしたいとき、MCMC(Markov Chain Monte Carlo)を使うことができます。
          <UL>
            <LI>ここでは、これを使って分割表の解の数を数えたいらしい
          </UL>
          <LI>MCMCでは、マルコフ連鎖の極限が、任意の分布をシミュレートできるようになっています。
          <LI>でも極限っていってもどのくらい待てばいいんだろう？ なにか、絶対にサンプリングできてるよ、みたいな方法はないのかなあ？
        </UL>
        <LI>解決法： パーフェクトサンプリングなら、有限の時間で確実に極限分布にしたがってサンプリングできますよ
        <UL>
          <LI><A href="http://ibisforest.org/index.php?%E9%81%8E%E5%8E%BB%E3%81%8B%E3%82%89%E3%81%AE%E3%82%AB%E3%83%83%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0">CFTP</A>(Coupling From The Past)という、超スゴイ方法を使えばこれができるらしい。
          <UL>
            <LI>マルコフ連鎖の全状態から進んでみて、全員が同じところに行き着いたらそれがサンプリングされた状態になっているよ、ということらしい。
            まだいまいちよくわからん。
          </UL>
          <LI>ポイントは、そのCFTPにおいて、マルコフ連鎖をうまくデザインするところ。
          で、そこで、うまくやりましたと。
        </UL>
        <LI><A href="http://www.simplex.t.u-tokyo.ac.jp/~kijima/documents.html">来嶋さんのページ</A>にフレンドリーな解説があるっぽい。
      </UL>
      <LI>鹿島さん（IBM）： ネットワークの構造予測 （12月にICDMで話す内容）
      <UL>
        <LI>問題： 欠損したネットワーク構造を、構造情報をもとに予測（復元）したい
        <UL>
          <LI>遺伝子のネットワーク構造や、SNSでの推薦、協調フィルタリングなどででてくる問題。
        </UL>
        <LI>解決策： ネットワークの時変モデルを使って推定するよ
        <UL>
          <LI>ネットワークの時間遷移モデルを考えて、パラメータ推定をする。
          <LI>変異の履歴は利用できないので、ネットワーク構造変異の定常状態を考えそれをフィットするという方法で解決。
        </UL>
      </UL>
    </UL>
    <LI>その他面白かった話＋α
    <UL>
      <LI>「僕的ベストペーパー」<A href="http://www.pa.info.mie-u.ac.jp/~takeuchi/index.html">竹内さん</A>（三重大）： 保険金の期待値を計算する
      <UL>
        <LI>問題： 保険の掛け金を計算するのに、各人に将来支払われるだろう保険金の条件付期待値を推定したい。
        <UL>
          <LI>保険金は higily skewed &amp; heavy tail だから、普通の2乗誤差回帰（期待値の推定）では推定値が安定しない。
          <LI>一方、絶対誤差を使った中央値などのパーセント点の推定はロバストだが、ホントに求めたいのは期待値である。
            
          <LI>困った。
        </UL>
        <LI>解決策： 2段階の回帰をやればロバストな推定ができるよ
        <UL>
          <LI>モデル化を2段階に
          <OL>
            <LI>条件付パーセント点を推定するモデル（絶対誤差、ロバストだけど目的違い）
            <LI>そこからのずれの期待値を推定する単純化されたモデル（2乗誤差、不安定）
          </OL>
          わける
          <LI>（１）は安定した推定ができるのである程度複雑なモデルをつくれる＋（２）の不安定さを、簡単化したモデルにすることで回避。
        </UL>
        <LI>実際にUSの保険会社からデータをもらってやったらしい。 
        <LI>実問題に根ざしているとともに理論的なバックグランドもちゃんとしている。（しかも英語もよどみなし。）
      </UL>
      <LI><A href="http://www.gssm.otsuka.tsukuba.ac.jp/professor/sato.html">佐藤さん</A>（筑波大学）： 商品の競合関係の解析
      <UL>
        <LI>問題： 新製品をいれると既存の商品との競合関係がどう変わるか知りたい
        <UL>
          <LI>売り上げの時系列モデリングを作ろうとすると、途中で次元（＝製品）が増えるのをどうしたらいいだろう？
        </UL>
        <LI>解決策： 次元切り替え付きの状態空間モデルを使うよ
        <UL>
          <LI>価格弾力性（価格を１単価下げると、売り上げがどれだけ上がるか）などのパラメータが事変するようにモデル化することで、これらがどのように変わるか知ることで知見を得られる。
            これらのパラメータが状態空間モデルの「状態」にあたる。
          <UL>
            <LI>状態の推定はカルマンフィルタで。
          </UL>
          <LI>新製品投入したときに、製品数が変わるから、合わせて次元が切り替わるようなモデルを考える。
          <UL>
            <LI>次元が増えるときには、既存のパラメータは現在値を事前分布として使う、逆に減るときには消える変数について周辺化する。
          </UL>
        </UL>
        <LI>結果：
        <UL>
          <LI>新製品が投入されると、既存商品の
          <UL>
            <LI>商品力が下がる（トレンドが下がる）
            <LI>値引きが効かなくなる（価格弾力性の定価）
            <LI>ブランド間差異（交差価格弾力性）が小さくなる
          </UL>
          などがわかる
        </UL>
        <LI>ちゃんと「知識発見」できてる！ 「実データ→モデル化→技→知識」と全体の流れがうまくまとまっているマーケティングサイエンスの良作。
      </UL>
      <LI>α（久保山さんたちとの共著論文）： 木のとても速い分類器
      <UL>
        <LI>問題： 木構造データの分類問題を高精度に、そして速く
        <UL>
          <LI>XMLや、糖鎖（バイオ分野）などは木構造で表される、これらのデータでは構造が意味を持っている
          <LI>鹿島&amp;小柳(2002)ではカーネル法という方法で「全ての考えうる部分構造」をベースに分類問題をそれなりに効率的に解く方法を考えたのだが、ちょっと遅い。
        </UL>
        <LI>解決法： 木のnグラムをベースに考えると速いよ
        <UL>
          <LI>木のnグラム（長さｎの配列と同型の部分構造）は、そこそこの表現力を持っているが、「全ての考えうる部分構造」よりも各段に速く数えることができる。
          <LI>配列のnグラムを使った<A href="http://www.google.co.jp/search?sourceid=navclient&hl=ja&ie=UTF-8&rls=GGLG,GGLG:2006-04,GGLG:ja&q=spectrum+kernel">spectrum kernel</A>に因んで、spectrum tree kernelちゅうことで。
        </UL>
      </UL>
    </UL>
  </UL>
  <LI><A name="20060919"></A>2006/9/19 現在、<A href="http://www.ecmlpkdd2006.org/">ECML/PKDD</A>が開催されているようだ
  <UL>
    <LI>その中の、<A href="http://www.inf.uni-konstanz.de/mlg2006/MLG2006.pdf">MLG（Mining and Learning with Graphs）</A>というワークショップ
    <UL>
      <LI>（コッソリ共著者に入れてもらっている）くぼやまさんが、木カーネルを、部分構造のマッピングの数とみることで、マッピングの種類を変える（曖昧な構造マッピングの程度の、曖昧さの程度を変える）ことでいろんな木カーネルのクラスがあることを示す。
      <LI>また、西郷さんたち（含津田さん）がベストペーパーをとったようだ。 めでたい。
      <UL>
        <LI>内容は<A href="http://www.icml2006.org/icml_documents/camera-ready/120_Clustering_Graphs_by.pdf">ICMLのやつ</A>を「化合物の部分グラフパターン→活性値みたいなやつ」という回帰に使ったようだ。
      </UL>
    </UL>
    <LI>PKDDのほうでは井手さんが、とある実験的時系列解析の大家と戦いを繰り広げる模様。
    <UL>
      <LI>戦いへの経緯の詳細は、いでさんのmixi日記に譲るが、<A href="http://spinglass.hp.infoseek.co.jp/ecml06-ide.pdf">発表の内容</A>は、k-meansとかで滑走窓で取り出された時系列片をクラスタリングすると、何故かクラスタ中心が正弦波になってしまう（＝つまらなくなる）という、とある大家が「実験的に」示した問題に対し、それが何故おこるかを「理論的に」示したもの。
      井手ワールドが炸裂しています。
      <UL>
        <LI>ちなみに、この論文の<A href="http://spinglass.hp.infoseek.co.jp/100171.pdf">日本語版短編</A>では人工知能学会の全国大会で賞もとっている。 めでたい。
      </UL>
    </UL>
    <LI>そのほか近況
    <UL>
      <LI>こっそり<A href="http://www.comp.hkbu.edu.hk/~wii06/icdm/?index=accepted">ICDMのAccepted Papers</A>が出ていた。
      <UL>
        <LI>日本人は計<S>4チーム</S>のようだ。<FONT color="#ff0000">（神嶌さんのご指摘、５チームでした）</FONT>
      </UL>
      <LI>北海道で来週開催される<A href="http://ibisml.org/dmss2006/">DMSS</A>に<A href="http://ibisml.org/dmss2006/program.html">招待講演</A>（？）することになった。 行ける口実ができてうれしい。 しかし、内容は先日のを大体使いまわせるとしても、英語、かあ…。
    </UL>
  </UL>
  <LI><A name="20060905"></A>2006/9/5 まさに結果オーライな
  <UL>
    <LI>ICDMに出していたリンク予測の査読結果が帰ってきた。 なんと&quot;regular
    paper&quot;としてアクセプトされた。
    <UL>
      <LI>正直、&quot;short paper&quot;になんとか引っかかってほしいと願っていたくらい。
      査読者の回答も&quot;reject&quot;方向の人は居ないにしても、かといって、そんなには褒めてない。
      <LI>しかし、ノイズを差し引いても&quot;short paper&quot;くらいのよさはあったのだろうということで、結果オーライです。
    </UL>
    <LI>結局、一番評価が高かったのが「presentation」。今回、<A href="http://www.research.ibm.com/dar/na-page.html">安倍さん</A>に論文の構成と英語の表現とかの「論文かくとこ」をだいぶみてもらったわけだが、纏め方・書き方というのは重要だなあ、というのは再認識。
    <LI>極端な話、書き方ひとつで通るものも通らなければ、通らないものも通してしまうのだとおもう。
    特に日本人の場合、うまく書けば、案外従来の3割増くらいの評価は得られるのだと思う。
    要するに、テクニカルな内容はそのままで、上手な人がもう一回書くと、通ってしまう論文は山ほどあるはず。
    <UL>
      <LI>そういえば、昔、<A href="http://www.kyb.tuebingen.mpg.de/main/staff.php?user=tsuda">津田さん</A>と一緒に書いてたとき「僕らはこの世界ではぺーぺーなんだから、一回読んで分かってもらえなかったら終わりだくらいの気持ちで書かないと」と仰っていた。
      んん、仰るとおり。
    </UL>
    <LI>内容を読むとイマイチなんだけど何故だかいいところにバンバン通っている人。
    そういう人は書き方も含め「何かしらの技」を身につけているのだとつくづく思う。
    <UL>
      <LI>ぜひ、そのへんをさぐっていきたいなあ…。
    </UL>
    <LI>ちなみに内容は、とあるパラメトリックなネットワークのモデルを考えて、リンク予測に使ってみたら、タンパク質ネットワーク構造の予測とか案外うまくいきましたよみたいな話です、が、ま、それはどうでもいいのです、結果オーライで。
  </UL>
  <LI><A name="20060824"></A>2006/8/24 訓練とテストの状況が違うときの学習
  <UL>
    <LI>通常、機械学習では、訓練とテスト（実地）は同じ状況で行われていることを前提としている。
    たとえば、入力xから出力yを予測するような教師あり学習でも、p(x)やp(y|x)は、訓練時とテスト時で変わらないとするのが普通である。
    <LI>しかし、実際には、さまざまな理由によりこれが成り立たない状況もある。
    <UL>
      <LI>偏ったサンプリングの問題： マーケティングにおいて、アンケートデータxをもとに、マーケティングに有益なとある属性yの予測をしたい場合、訓練データにはそもそもアンケートに答えてくれた人のデータしかない。
      つまり、実際の世の中のp<SUP>test</SUP>(x)と、訓練データのp<SUP>train</SUP>(x)が異なっている。
      <LI>ドメインの違いの問題： テキスト処理において、ニュースの文書xから、そのカテゴリyを当てたい場合、時期によってp(x)が変わってくる。
      あるいは、ある製品の感想xから、その製品に対する意見の種類yを当てたい場合、同じ単語（例えば「短い」）でも分野によってイミの良し悪しが異なってくる。
      この場合p(y|x)が変わったといえる。
    </UL>
    <LI>そんなときにどういうやり方が適用できるかは、テストの状況やデータがどれだけ分かっているかに依存する。
    <UL>
      <LI><A href="http://sugiyama-www.cs.titech.ac.jp/~sugi/2006/covariate-shift-jp.pdf">杉山さんのナイスサーベイ</A>では、テスト時のp(x)が分かっている場合にこれを活用する方法が紹介されている。
      <UL>
        <LI>importance samplingと同じ考え方で、訓練データがテスト時の分布で生成されたかのように、訓練データをぐいっと重み付けして学習する。
        <UL>
          <LI>訓練データxに対し、p<SUP>test</SUP>(x)/p<SUP>train</SUP>(x) で重み付けする。
          <LI>さらにここで (p<SUP>test</SUP>(x)/p<SUP>train</SUP>(x))<SUP><FONT size="-2">λ</FONT></SUP>などとやったりするのがオシャレらしい。
          <UL>
            <LI>このやりかたって、直接目的関数の勾配をとるタイプの強化学習にもそのまま使えそうですな。
          </UL>
          <LI><A href="http://scholar.google.com/scholar?q=%22one+benefit+learning%22&ie=UTF-8&oe=UTF-8&hl=ja&lr=">ビアンカのやつ</A>も本質的には同じことをやっている。
        </UL>
      </UL>
      <LI><A href="http://www.jair.org/media/1872/live-1872-2703-jair.pdf">コレ</A>とかは、テスト時の(x,ｙ)両方がある程度与えられている状況。
      <UL>
        <LI>こういう話は自然言語処理ではdomain adaptationという名で研究がされている模様。
        テスト＝適用したいドメイン内のテキスト、訓練＝世の中にたくさんあるテキスト、みたいな対応。
        <LI>リンク先の論文は、「ドメイン内」、「一般」、「ドメイン外」、の3つの分布からデータが発生していると考えて、「適用先のドメインのデータ」は前者の2つから、「世の中にあるデータ」は後者の2つから発生していると考えてEM、というアプローチ。
        <UL>
          <LI>なお、この論文では先行研究として、一般のテキストから学習したパラメータを事前分布の平均だとして使って、ドメインのデータで学習する、というやり方が紹介されている。
        </UL>
      </UL>
    </UL>
    <LI>テスト時の情報が事前に与えられていなくて、訓練時と一緒かもしれないし、ちょっと違うかもしれない、みたいな問題もありうると思う。
    <UL>
      <LI>こないだ読む会で紹介されていた<A href="http://www.icml2006.org/icml_documents/camera-ready/045_Nightmare_at_Test_Ti.pdf">Nightmare 
at Test Time: Robust Learning by Feature Deletion</A> もテスト時にfeatureが部分的に消えることを想定してロバストな予測器を作るというので近い話かも。
      <LI>そいえば、<A href="http://www.stat.berkeley.edu/users/chenchao/666.pdf">random forest</A>というやつ、僕は知らなかったのだが、これもfeatureを適当に選んだいくつかに限って作った決定木を組み合わせるという、featureのbaggingぽいことをやってるらしいが、関係あるのかもしれない。
      <UL>
        <LI>ちなみに<A href="http://citeseer.ist.psu.edu/284770.html">featureの上でのboosting</A>なんてのもあるらしい…。
        <UL>
          <LI>読もうとしたけど、なんかヒューリスティクスっぽくてよくわからんかった。
        </UL>
      </UL>
    </UL>
  </UL>
  <LI><A name="20060803"></A>2006/8/3<A href="http://www15.atwiki.jp/tsuboi/pages/4.html"> ICML読む会</A>
  <UL>
    <LI>先日、7/29（土）に、東工大すずかけ台で場所をお借りしてICML読む会をやってみた。
    <LI>まず、実際にICMLに論文を出してる2人の論文紹介
    <UL>
      <LI>杉山先生の「<A href="http://www.icml2006.org/icml_documents/camera-ready/114_Local_Fisher_Discrim.pdf">Local 
Fisher Discriminant Analysis for Supervised Dimensionality Reduction</A>」は、フィッシャー判別をするときに、あるクラスのデータが多峰性をもつ（例えば、正例のクラスタが二つに分かれてて
      正例-負例-正例みたいな感じで固まっているとか）場合にうまくいかないのを、クラス内分散を近いペアについてのみで定義することで解決する。
      イイところはもともとのフィッシャー判別と同じように固有値の問題で解けちゃうところ。
      うむ、カッコイイ。
      <LI>工藤さん（と津田さん）の「<A href="http://www.icml2006.org/icml_documents/camera-ready/120_Clustering_Graphs_by.pdf">Clustering 
Graphs by Weighted Substructure Mining</A>」はグラフのクラスタリング。 すべての部分グラフをfeatureに使った混合分布モデルのEM推定なのだが、L1ノルムを使って解をスパースにする（＝実際に使用する部分グラフが少なくてすむ）のと、部分グラフを見つけるのにグラフマイニングを使うのが見事に融合。
      うむ、カッコイイ。
    </UL>
    <LI>そして、そのほかの参加者が論文を紹介、いくつか心にヒット。
    <UL>
      <LI>「<A href="http://www.icml2006.org/icml_documents/camera-ready/026_Trading_Convexity_fo.pdf">Trading 
Convexity for Scalability Predictors</A>」は、SVMのhinge loss (凸関数) に、lossが大きいときにある程度以上大きくならないように平らにする（＝過学習を防ぐ）ための補正
      （凹関数）を足した目的関数を最適化する（CCCP：ConCave-Convex Programmingとか言っている）。
      目的関数は凸ではなくなるが、凹関数部分を勾配で近似してやりながら、凸最適化を繰り返しとく形にもっていく。
      この手の「凸じゃなくても、やりたいことをより正しくあらわしている目的関数なら使っていこうよ」的な考え方は最近ちょっと盛り上がっているようだ。
      うむ、カッコイイ。
      <LI>「<A href="http://www.icml2006.org/icml_documents/camera-ready/083_Online_Decoding_of_M.pdf">Online 
Decoding of Markov Models under Latency Constraints</A>」は、オンラインでViterbi decodingする（＝観測系列の全体をみることなしに、ある時点の隠れ変数を当てる）必要がある場合、に当てたい時点から何個先まで観測すれば、当てたい時点の隠れ変数を当てられるか、を考えたもの。
      遅れと曖昧さをトレードオフした目的関数が最小になるような最適な遅れを決める。
      これって、リアルタイムでの異常検知や自然言語処理に需要のある重要な問題だとおもう。
      うむ、カッコイイ。
      <LI>ちなみに僕の紹介した「<A href="http://www.icml2006.org/icml_documents/camera-ready/133_Discriminative_Unsup.pdf">Discriminative 
Unsupervised Learning of Structured Predictors</A>」は、structured output を教師ナシでやる話。 なんじゃそりゃと、すごく気になったので読んでみたのだが、基本は著者らが前からやっていた教師ナシのSVM
      （＝マージンが最大になるようなクラスラベルの割り当てを決める。 SDPになる。）を拡張していったもの。
      経緯がわかるとまあそんなもんかな、という感じ。 実験データがものすごく小さいので、相当遅いのだろう。
      うむ、これは微妙？
    </UL>
    <LI>会自体は結構盛り上がって楽しかった。 こんどはNIPS読む会を、ってことに。
  </UL>
  <LI><A name="20060720"></A>2006/7/20 学生のときに何をやっているのかよくわからずにやっていたものをコッソリ振り返ってみる（３）
  <UL>
    <LI>今回は、<A href="http://groups.yahoo.co.jp/group/bioinformatics-jp/message/3092">例の講義</A>の３回目、情報理論。 物理の学生向けに一般的な話ということだったが、 ええ、復習というより、かなり普通に勉強になりました。
    <UL>
      <LI>ちなみに今回の講義の資料が<A href="http://www.ism.ac.jp/~iba/kougi_2006_ism/cyuo.html">ココ</A>で入手できます。
    </UL>
    <LI>（情報論的な）エントロピーの解釈についてひとつ面白い話があった（もしかしたら、普通に授業で習うのかもしれんですが…）ので、ご紹介。
    <LI>情報論的エントロピーは - Σp(x) log p(x) で定義されるが、これは見方によると「確率の期待値」（正確には「対数をとった確率の期待値」）として解釈することができる。
    いいかえると「ある確率分布によって生成される事象の平均的生起確率はどのくらい？」というのをあらわしていると。
    <UL>
      <LI>「確率の上の確率分布」を考えるてのが、ちょっとトリッキーぽいけど、物理の人だと、個々の事象ではなく、あるエネルギーをとる事象の集合を考えてもともとの事象→エネルギーに置き換えてしまい、そのうえで「エネルギーの期待値」を考える、というのに慣れているらしいので、そのイメージらしい
      （一個目の「確率」を「エネルギー」に置き換えて考える。）。 たぶん「典型的なエネルギーってどのくらい？」みたいな感じ。
      <LI>同じように、エントロピーは「典型的な確率ってどのくらい？」みたいな量だと。
      平均は、ある確率変数の特徴を表すものだから、いま、ある確率分布の特徴量としてエントロピーを定義しようとすると、「確率」という確率変数の平均をとる、というのは自然であると。
    </UL>
    <LI>Z(x) = log p(x) を確率変数だと思うと、エントロピーはその期待値になっているので、こいつをp(x)について平均したものがエントロピーであると。
    じゃあ「なんでlogをとるのか？」というと、大体p(x)というのは、複数の確率の積（xは大体ある種の系列で、それは、（条件付の）サイコロを何度も振って作られる）なので、普通に期待値を取っちゃうとほとんど確率0のところに確率がかたまってしまい、平均が意味をなさなくなってしまう。
    こういうときにはlogをとるのが定石で、そうすることで積→和になってlog p(x)が対数正規分布に従うから（たぶんな）。
    <LI>妄想。
    <UL>
      <LI>そんな風に考えてみると、じゃあ、分散とかのもうちょっと高次のエントロピーも考えられるんかなあ。
      <UL>
        <LI>q(p) を確率ｐをとる確率とすると、- Σp(x) log p(x) = -∫q(p) log p dp だから、こんな
        -∫q(p) (log p)<SUP>d</SUP> dp かんじ？
      </UL>
      <LI>確率分布の特徴を表すために、確率の上の確率分布q（＝ある確率で発生する事象がどのくらいあるか）によって表すのは、各事象がどれだけの確率で起こるのかを表すpよりももう少し抽象的な情報であるといえる気がする。
      <LI>たとえば、p(グー)=0.5、p(チョキ)=0.25、p(パー)=0.25 という確率分布も、p(晴れ)=0.25、p(雨)=0.25、p(曇り)=0.5
      という確率分布も定義されているドメインは違うけど、確率分布の性質としては同じはず。
      <LI>２つの確率分布の違いを表すのに、よくKL-divergenceを使ったりするわけだが、これは２つの分布が同じドメインに属していないと意味ない。 でも、確率の上の確率分布ってとこまで抽象化してしまえば、全く違うドメインで定義された確率分布を比較することが出来るはず？
    </UL>
  </UL>
  <LI><A name="20060719"></A>2006/7/19 おこがましさ
  <UL>
    <LI><A href="http://www.donald.ai.kyutech.ac.jp/~hiroshi/">坂本さん</A>つながりで<A href="http://www-ikn.ist.hokudai.ac.jp/sigfpai/next.html">9月8-9日に九州である人工知能学会の研究会</A>の招待講演でネットワーク構造の解析についてのお話をさせていただくことになった。
    <LI>対象ははやりものの<A href="http://www.google.co.jp/search?sourceid=navclient&hl=ja&ie=UTF-8&rls=GGLG,GGLG:2006-04,GGLG:ja&q=%E8%A4%87%E9%9B%91%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF">複雑ネットワーク</A>とかなんとかの類だけど、機械学習の立場（たぶん、不均質な対象についての予測のような）からの問題設定とかアプローチとかを紹介しようかと。
    ていうか、<I>勉強して</I>、紹介しようかと。
  </UL>
  <LI><A name="20060714"></A>2006/7/14 MCMCに萌え
  <UL>
    <LI><A href="http://groups.yahoo.co.jp/group/bioinformatics-jp/message/3092">メーリングリストで流れてきたモンテカルロな講義</A>が公開だったので行ってみた。
    <UL>
      <LI>講師は、例の<A href="http://www.amazon.co.jp/gp/product/4000111582/">ベイズのすばらしい入門</A>を書かれた方。
    </UL>
    <LI>基本的には、ある分布に従うサンプルをとりだす方法についての話。 たとえば、ベイズとかだと、事前分布に従うパラメータをばんばんサンプリングするのに使う。
    この手の話はGibbs samplingくらいしか知らなかったので勉強になった。
    <LI>いつの日か求まる系の話はあまり好みではなかったのだが、拡張アンサンブルの話とかは技的にもすごく面白いと思う。
    ある制約を満たす解の数え上げ（たとえば、魔方陣の数を数える）に使えるというのはすごい。
    <UL>
      <LI>何か面白そうな使いかたをしてみたいなあ。
    </UL>
    <LI>ひどく感銘をうけたので、とりあえず、<A href="http://www.iwanami.co.jp/moreinfo/0068520/top.html">でかい方の本</A>も買った。
  </UL>
  <LI><A name="20060711"></A>2006/7/11 リンク予測
  <UL>
    <LI>ひさしぶりの更新。 リンク予測についての論文を<A href="http://www.comp.hkbu.edu.hk/~wii06/icdm/">ICDM</A>に出したので、そのまわりの話を書いてみる。
    <UL>
      <LI>うまいこと引っかかるといいんだけど、48時間前で７００台のIDが振られているので、かなりアレだなあ。
    </UL>
    <LI>リンク予測は、ネットワーク構造を予測する問題（下のほうで紹介したﾔｽｵ問題）で、
    <UL>
      <LI>入力： ノードの集合。 いくつかのノードペアについて、その間に枝があるか／ないかの情報
      <LI>出力すべきもの： 残りのノードペアについての枝の有無
    </UL>
    あるいは、現在のネットワーク構造は完全に分かっているとして、そこから将来の（次の時点での）構造を予測する問題。<BR>
    ジャンルとしては、最近は「リンクマイニング」の中のひとつのタスクとして位置づけられるようだ。
    <LI>基本的には、２つのノードの間に枝があるか／ないかというのを当てる、ノード<U>ペア</U>の２値分類、あるいは、ありそっぷりの順にならべたランキングの問題になる。
    <LI>アプローチとしては、たぶん一番一般的に捉えると、枝も入れたマルコフ確率場で考えるのが正解。
    TaskarがRelational Markov Networkというのを提案している。 でも、それだとちょっとでかすぎるというか、もうちょっと深入りする方向で考えてみる。
    <LI>じゃあ、分類問題なんだから、まず特徴（feature）を定義しなきゃねということで「枝の有無の予測に使える特徴は何か？」と考えると多分２種類あるだろう。
    <OL>
      <LI>各ノードの持つ特徴
      <UL>
        <LI>たとえばSNSだったら、ノード＝個人なので、その人の年齢とか、住んでるところとかがコレにあたる。
      タンパク質のネットワークなら、ノード＝タンパク質なので、そのタンパク質の配列とか、発現量とか。
        <LI>ただし、注意しなければならないのは、やりたいのは各ノードの分類ではなく、ノードペアの分類なので、この特徴もノードペアに対して定義される。
        たとえば、ここでカーネル法を使おうとするなら、ノードペアとノードペアの間のカーネルを設計することになる。
        この辺は↓の方で述べたとおり。
      </UL>
      <LI>構造のもつ特徴 （今回はこっちに注目したい）
      <UL>
        <LI>これは今リンクを予測しようとしている２つのノードのまわりの構造的な特徴を捉えたもの。
        <LI>Liben-Nowelly &amp; Kleinbergの「<A href="http://scholar.google.com/scholar?hl=ja&lr=&cluster=8420143622481885924">The Link Prediction Problem for Social Networks</A>」は、この手の特徴がどのくらい予測力があるのかを共著関係のネットワークに対して調べている。
        <LI>たとえば、「２つのノードが共通にもつ隣接ノードの数」とか「２つのノードの隣接ノード数の積」とか。この手の特徴は大抵、対応する「ネットワーク構造の形成／変化モデル」にインスパイアされている。
        たとえば、前者は「共通の隣接ノードが多いほどその２つのノードの間に枝がはられやすい」ようなモデル（友達の友達は友達みたいなやつ）、後者は例の有名なpreferential
        attachment。
      </UL>
    </OL>
    で、こういう特徴を全部使って教師アリ学習しましたよ、ってのが<A href="http://scholar.google.com/scholar?hl=ja&lr=&q=%22Link+Prediction+using+Supervised+Learning%22&lr=">こういうやつ</A>。 まあ、どっちかというと半教師アリ（むしろトランスダクション？）、あるいは「positive
    and unlabeled examples」な問題ではあるんだけど…。
  </UL>
  <LI><A name="20060607"></A>2006/6/7 学生のときに何をやっているのかよくわからずにやっていたものをコッソリ振り返ってみる（２）
  <UL>
    <LI>ちゃんと学生のころに勉強してたひとには自明だけど、実はコッソリ自明じゃないことが色々ある人もたくさんいるはず（含僕）。
    そんなあなたに最小二乗法からFFT、ウェーブレットまで<A href="http://www.amazon.co.jp/exec/obidos/ASIN/4320017382/">コッソリ教えてくれる</A>。
    <UL>
      <LI>ええ、僕、FFT知りませんよ。
    </UL>
    <LI>計算もいちいち載っていて電車でも追える、オトナの皆様にも親切設計。
    <LI>あと、コラムで先生と生徒の対話というありがちなパターンがあるのだが、よくある「おそろしく物分りのイイ弟子が、先生の発言を補う気の利いたセリフを吐く」という表面だけがソフトで、むしろ邪悪な解説（よくあるよね…<A href="http://怖くてリンク張れません">アレ</A>とか<A href="http://すいません、やっぱり張れません">アレ</A>とか…）とかとは違い、いい感じで出来の悪い生徒の「正規分布とかいわれても授業に出てなかったのでわかりません！もうちょっと分かりやすく言ってください！」的逆ギレ発言に対し先生が「とりあえずは気にするな」的宥め発言をするあたりのぶっちゃけっぷりが共感！
  </UL>
  <LI><A name="20060605"></A>2006/6/5 入手可能なネットワークデータ×3
  <UL>
    <LI>social networkのような構造（外部構造というやつ）を解析する方法を試したいときに、実際のネットワークデータが欲しいわけだが、何かお手軽に試せるデータはないかな…ということで、次の３つは気軽に使えそう。
    <LI>social network 解析の有名らしい教科書の中でつかわれているデータが<A href="http://www.insna.org/INSNA/data_inf.html">ココ</A>で公開されている。 最大ノード数50くらいなので小さいが、とりあえず動くことを示すのには使えそう。
    <LI><A href="http://web.kuicr.kyoto-u.ac.jp/~yoshi/">山西さん</A>の論文で使われているタンパク質の相互作用ネットワークのデータが<A href="http://web.kuicr.kyoto-u.ac.jp/~yoshi/ismb04/">ココ</A>で公開されている。 social networkの文脈で使えるかは微妙だけど、ノード数が700くらいなのでとても使いやすい。
    <LI>もうちょっと大きいデータだと、過去のNIPSでの共著関係などのデータが、<A href="http://ai.stanford.edu/~gal/data.html">ココ</A>で入手できる。 論文x著者の行列データなどが入っている。
    <UL>
      <LI>MATLAB形式なのがちょっと迷惑だけど、著者をノードだと思うと3000弱のノードがあり、それなりの大きさ。
    </UL>
    <LI>そういえば以前<A href="http://www.itmedia.co.jp/news/articles/0509/14/news040.html">mixiのデータを解析する祭り</A>があったようだが、どうもこれっきりデータを外には出さないみたいな話を聞いたような気がするのだが、実際どうなんだっけ？
  </UL>
  <LI><A name="20060523"></A>2006/5/23 学生のときに何をやっているのかよくわからずにやっていたものコッソリを振り返ってみる（１）
  <UL>
    <LI>「<A href="http://www.amazon.co.jp/exec/obidos/ASIN/4339031402/">自動制御とは何か</A>」：見た目は教科書だが、ブルーバックスのりの式のない本。 制御工学の歴史に、フィードバック制御から、古典制御理論の確立、現代制御理論の概要などを含めた制御工学全般の紹介。
    <LI>蒸気機関の安定化など産業上の要請から生まれ、はじめはまさに試行錯誤のアートだったのが、次第に数学的な理論が整備され、体系化されていくさまがカッコイイ。
    少なくとも前世紀には、そうとうイケてる研究分野だったのだと思う。
    <LI>ところで制御といえば、先週、<A href="http://www.ri.cmu.edu/people/kanade_takeo.html">金出先生</A>がいらっさり、講演を聴いた。 ベースは「<A href="http://www.amazon.co.jp/exec/obidos/ASIN/4569662870">シロクロ</A>」の話。
    <UL>
      <LI>大抵、空回りして「クロクロ」になっちゃうんだろうけど、それだったら「シロシロ」のほうが全然マシなんだろうね。
      <LI>ていうか、なによりも、大事なのは、やっぱ体力／元気、と。 んー、足りんなあ。
    </UL>
  </UL>
  <LI><A name="20060522"></A>2006/5/22 <A href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUC</A> optimization
  <UL>
    <LI><A href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC</A>(Receiver Operating Curve)は、たとえば、２値分類問題を考えるとすると、入力x
    に対し、f(x)+b&gt;0 なら正クラス、&lt;0 なら負クラスというように、符号で分類しようとしたときに、bの値を変えながら、横軸に(テストデータに対する)
    <A href="http://en.wikipedia.org/wiki/False_positive">false positive</A> rate、縦軸に(テストデータに対する) true positive rateを<A href="http://images.google.co.jp/images?sourceid=navclient&hl=ja&ie=UTF-8&rls=GGLG,GGLG:2006-04,GGLG:ja&q=ROC%20curve&sa=N&tab=wi">プロットした点を曲線でつないだもの</A>。
    <UL>
      <LI>これが上にあるほど良いアルゴリズムとされる。
    </UL>
    <LI>このカーブ以下の面積はAUC(Area Under ROC)とよばれ、学習手法の評価基準として使われる。
    AUCは、正例に対するf(x)の値が負例に対するf(x)の値よりも大きくなる確率（fは固定で、xの分布に対して）をあらわしている。
    したがって、AUCを大きくすることを狙って学習を行うには、この確率を訓練データに対して最大化すればよいことになる。
    <UL>
      <LI>AUCは以下の手続きによってもとまる。
      <UL>
        <LI>テストデータに対してf(x)の値を大きい順にならべる。AUC:=0、TP:=0とする。
        <LI>f(x)の大きいほうのxから順にみていく。
        <UL>
          <LI>xのクラスが正なら<S>AUC:=AUC+1</S>、TP:=TP+1。
          <LI>xのクラスが負なら、AUC:=AUC+TP。
        </UL>
        <LI>最後にAUCを、（正のテストデータ数）×（負のテストデータ数）で割る。
      </UL>
      <LI>要するに上からみていって、正のデータが先行すればよい。 つまり正正正…正負負負…負、となればベスト。
      <UL>
        <LI>負例のランクが正例の数より上に食い込んだぶんだけロスになる。
        <LI>これを最適化する問題はランキングの問題として考えることができ、負例のランクの和の最大化、あるいは、正例のランクの和の最小化、などによって達成できる。
      </UL>
    </UL>
  </UL>
  <LI><A name="20060519"></A>2006/5/19 でてる
  <UL>
    <LI><A href="http://www.siam.org/meetings/sdm06/proceedings.htm">SDM06の論文が全部</A>みれる。
    <LI><A href="http://www.icml2006.org/icml2006/technical/accepted.html">ICML06のAccepted Papers</A>がでてる。
    <UL>
      <LI>あ、また津田さん通ってるよ…。
    </UL>
  </UL>
  <LI><A name="20060514"></A>2006/5/14 Yet Another Structured Output (ﾔｽｵ)
  <UL>
    <LI>「構造をもったデータ」の「構造」には、internal structure（内部構造）とexternal
    structure（外部構造）がある。
    <UL>
      <LI>内部構造は、XMLや、構文木、タンパク質の配列などのように、1つのデータの中にグラフ構造があるタイプ
      <LI>外部構造は、ソーシャルネットワークやタンパク質の相互作用ネットワークのように、データ同士の関係をグラフ構造で表現したもの。
    </UL>
    <LI>通常、structured outputな問題というと、内部構造をもったデータから内部構造をもったデータへのマッピング（e.g.
    文字列→構文木） を学習する問題をさす。これをinternally structured outputな問題と呼ぶことにすると、部分的に明らかになっているネットワークからネットワーク全体を予測する問題は、externally
    structured outputな問題と呼ぶことができるだろう。
    <LI>って、そうだよなーって思っただけなんだけどね。
  </UL>
  <LI><A name="20060510"></A>2006/5/10 (´･ω･`)ｼｮﾎﾞｰﾝ
  <UL>
    <LI>JMLRに出していた論文がガツンとreject。
    <UL>
      <LI>コメントもごもっとも。
    </UL>
    <LI>最近、2枚目の壁にぶち当たってる気がするなあ。
    <UL>
      <LI>全部で何枚あるのかは知らんけど。
    </UL>
    <LI>ところで、ふと、おもったのだけれど、「チャンス発見」とかいう話は、フォーマルに扱うとすると、要するに分布の裾野に注目しなさいということになるのだろうなあ。 悪いほうの裾野がリスクで、良いほうの裾野がチャンスだ、と。
  </UL>
  <LI><A name="20060502"></A>2006/5/2 「機械学習とゲーム理論」は何を研究しているのか
  <UL>
    <LI>最近も流行っているかどうかは疑問だが、ゲーム理論のフレーバーを機械学習に持ち込むような話がいくつかある（あった）。
    <LI>僕の眺めた範囲を総合すると、どうやら少なくとも１つの話題としては「利得行列みたいなものがわからないときに、実際にアクションをとってみたりしながら、何がしかよい行動ルールなり均衡点なりを学習する」というセッティングの話があるようだ。
    <LI>たとえば、以前紹介した「<A href="http://scholar.google.com/scholar?q=%22adaptive+game+playing+using+multiplicative+weights%22&ie=UTF-8&oe=UTF-8&hl=en">オンライン学習とゲーム理論</A>」は、あるプレイヤーの立場から最適な行動ルールを学習する。
    <UL>
      <LI>これはいろいろ応用があるだろう。 ていうか、こっちは別にゲーム理論とつながる実益はないかもしれない。
    </UL>
    <LI>もう少し上の視点からみると、「<A href="http://citeseer.ist.psu.edu/hu98multiagent.html">マルチエージェント強化学習とゲーム理論</A>」、これはみんなでナッシュ均衡とかを探す。
    <UL>
      <LI>こっちは、何の役に立つのかというのは疑問の余地はあるけど、おしゃれな感じはする。
      <UL>
        <LI>「社会をエージェントシステムと考えたとき〜」云々みたいなおしゃれトークはできるかもしれないけど、もうちょっと地面に近いレベルでなにか有益なことをやって見せられたらよいのだろうなあと思う。
        やっぱルーティングとか？
      </UL>
    </UL>
  </UL>
  <LI><A name="20060428"></A>2006/4/28 <A href="http://local.google.co.jp/local?f=q&hl=ja&q=White+Plains,+NY,+USA&ll=41.212238,-73.802762&spn=0.014689,0.036092&om=1">ワトソン研究所</A>滞在
  <UL>
    <LI>プロジェクトの内容をむこうの人と詰めるため、SDMのあとに1週間ほど滞在。
    <LI>仕事のほかは興味の赴くまま、いろんな人の部屋にいって話を聞く。 やはりみんな、それなりにスゴイ。
    凄そうなひとは当然かなり凄いけど、凄くなさそうだった人も実はそれなりに凄い。
    ビジネスマンタイプにしろ、理論タイプにしろ、実践タイプにしろ、基本的にレベルもモラルも高い気がする。
    そろそろ「研究」の世界に足を踏み入れて10年が近くなろうとしているわけだが、なんかもうちょっとなあ、がんばらないとなあ、という気分になりました…。
    <LI>自分の研究紹介（SDMのやつ）をするセミナーは無事終了。 微妙にごまかしているところとか、みごとに突っ込まれる。そそそ、そこの凸とかいってるとこ、誤魔化してるんですよ！あたり！
    <LI>1週間を通じて<A href="http://domino.research.ibm.com/comm/research_people.nsf/pages/nabe.index.html">安倍さん</A>には公私ともにお世話になる。 もう、ついて行きますよと。
  </UL>
  <LI><A name="20060426"></A>2006/4/26 <A href="http://www.econ.uiuc.edu/~roger/research/intro/rq3.pdf">quantile regression</A>
  <UL>
    <LI>入力出力ペアの訓練データから、入力からquantileを予測するモデルを学習する話
    <LI>たとえば、顧客プロファイル（入力）と、その人が自社のサービスに払ったお金（出力）が訓練データとして与えられたとする。 ここから、顧客プロファイルが与えられると、その人が「最大いくらぐらい払う可能性がありそうか（お財布にいくら入っているか）」を予測したい。
    <UL>
      <LI>もうちょっとちゃんというと、ある小さいpに対し、100p％の確率で、いくらぐらいなら払ってくれそうか。
    </UL>
    <LI>たとえば、普通に平均値を予測するなら、yを真の出力、y'を予測値として、2乗誤差
    (y-y')<SUP>2</SUP>とかを使ったりするわけだが、quantile regression では、この誤差を次のように定義することで、quantileを予測するモデルが作れる。
    <UL>
      <LI>y≧y'のとき： p (y-y')
      <LI>y&lt;y'のとき： (1-p)(y'-y)
      <LI>絶対誤差を最小化すると中央値（0.5-quantile）が推定できるので、絶対誤差を右と左で重みを変えることでどっちかにぐいっとずらす感じ。
    </UL>
    <LI>たったこれだけでいいなんて面白いなあ。
    <LI>これを顧客のお財布の中身推定につかったのが、<A href="http://www-stat.stanford.edu/~saharon/papers/wallet-final.pdf">これ</A>。
  </UL>
  <LI><A name="20060423"></A>2006/4/23 【可読ではない】 SDM終了
  <UL>
    <LI>詳細はあとでまた詰めるとして、とりあえず、ざざっとメモを。
    <LI>会議はわりとこじんまり、200人いないんじゃないかな。 内容はICMLとICDMの間くらいの雰囲気？
    日本人は計3名。
    <LI>チュートリアル
    <UL>
      <LI>行列計算 （もとになる資料は<A href="http://cs-www.cs.yale.edu/homes/mmahoney/matrix/index.html">このへん</A>だとおもう）
      <UL>
        <LI>これはナイス。
        <LI>ばかでかい行列の計算（掛け算とか、分解）をやるのに、行列全体を扱いたくないので、行や要素をサンプリングで選んで近似的に計算する話。
        <LI>ちゃんと理論的保証があるんだなあ。
      </UL>
      <LI>semi-supervised な話
      <UL>
        <LI>結局、制約つき（semi-supervised）クラスタリングの話
        <LI>semi-supervised clusteringには２つの路線があります。制約をいれる方法（must-link,
        cannot-link）と、距離を学習（次元の重み付けとか）するしくみをいれる方法。
        <LI>KDD04のベストペーパーだったHMRFはこの問題を解いていたらしい。HMRFは、各データがどのクラスタに入るかの変数の値が隠れていて、これらのあいだに、制約が表現されている。
        表面に見えているのは、各データに対応する文書ベクトル。 文書ベクトルの生成モデルはクラスタ変数の値に依存する。
        EMで学習。
        <LI>内容より、隣でツメをがりがり齧りながら前後左右にブルンブルン上半身を振るガイのおかげですっかり気分を害しました。
        爪切り貸したろか、と。 まあ、まえに座ってるオッサンもでかすぎるんだけどね。
        何メートルあるんだよと。
      </UL>
    </UL>
    <LI>発表
    <UL>
      <LI>AUC optimization
      <UL>
        <LI>解く問題： 分類
        <UL>
          <LI>AUC（ROCカーブの下側の面積）を直接最適化する話。 discriminative派としては、正しい方向。
          <LI>従来との差分がよくわかんないけど、feature selectionしましたってあたり？
        </UL>
        <LI>ところで、AUCが、2つの正例と負例に対する確率の差の期待値であらわされるってのは、よくわかってないが、へえ。
      </UL>
      <LI>transform regression
      <UL>
        <LI>解く問題： 回帰
        <LI>ワトソンのおじさん。DB2なんとかマイナーに入っている、とのことなので、たぶん、実用性は高いのだろう。
        <LI>アプローチ：
        <UL>
          <LI>gradient boostingで回帰モデルをちょっとづつ足していく。
          <LI>しかし、それだけだと、収束が遅いので、前のラウンドまでの回帰モデルを変数にいれたり、毎ラウンド混合パラメータを最適化したりとかで速くしているもよう。
        </UL>
      </UL>
      <LI>LDA for unsupervised entity resolution
      <UL>
        <LI>research trackのベストペーパー。
        <LI>解く問題： 共著の関係から、同一人物を括る問題。
        <LI>アプローチ： 研究グループが潜在的にあって、そこから実際の共著関係が出てくる、という感じのモデル。
        <UL>
          <LI><A href="http://hillbig.cocolog-nifty.com/do/2006/05/a_latent_dirich.html">ここ</A>に説明が…。
        </UL>
        <LI>このセッティングで、どのくらい汎用性があるのかは不明。 逆にいろいろなセッティングに拡張できるんじゃない？
      </UL>
      <LI>A random walk method for text classification
      <UL>
        <LI>解く問題： グラフの上のsemi-supervised 学習。 データがノード、グラフはそれらの間の関係を表す。
        <UL>
          <LI>グラフの上で、ラベルつきノードと、ラベルなしノードが与えらる。 目的はラベルなしノードのラベルを当てる。
        </UL>
        <LI>アプローチ： グラフの上をラベルが伝播していく、といういつものアプローチだが、通常と違いそうな点は、こんなかんじかな。
        <UL>
          <LI>ラベルつきノードが、マルコフ連鎖のsink（入ったら出てこれないノード）とされ、ラベルなしノードからスタートしたランダムウォークが、正例のノードに吸い込まれるか、負例のノードに吸い込まれるかの確率によって分類する。
          <LI>遷移行列（を定義するためのノード間重みの行列）が対称でなくてもいい。 対称なラプラシアンを導くことができる。
          <LI>新しいノードが追加されたときに、追加されたノードに関わる行列の部分の計算だけによって予測が行える。
        </UL>
      </UL>
      <LI>シミュレーターからの知識発見
      <UL>
        <LI>解く問題： シミュレーションがうまくいくようなパラメータの領域をみつける
        <LI>課題： シミュレーションが重いので、しらみつぶしにパラメータを調べることはできない
        <LI>アプローチ： 能動学習。 怪しいパラメータ設定を見つけて、シミュレーションを走らせてみる。
        得られた結果をもとにｆ：パラメータ→結果 の学習を行い、次に怪しいパラメータを決める。
      </UL>
    </UL>
    <LI>ポスター
    <UL>
      <LI>じぶんのポスター発表
      <UL>
        <LI>別にこれといった華々しい反応もありませんでした。 ご自由にどうぞって感じで吊るしといた資料がだいたいはけたのは救い。
        誰も取らなかったらどうしようかとおもてたよ
        <LI><A href="http://www-stat.stanford.edu/~saharon/">ワトソンのサハロンとかいう兄ちゃん</A>にからまれる？
        <UL>
          <LI>なんかポスターにちょろっと書いてた詳細にカチンときたのか、分類哲学みたいのをとうとうとしゃべられた。
          <LI>「やばい、からまれた？」とおもたら、そのうち興味をもてもらえたらしく、とりあえずワトソンでまた、とまるく収まる。
          <LI>いろいろと説明してくれて、なんだイイ奴だな、とおもったら、僕がGradient
          Boostingを使うときに参考にした論文は、実は彼が書いたものでした…。 あと、このひとFriedmanやHastieのとこに居たのね…。
          <LI>ところで、彼のポスターの話は、求めたい分布とはちょっと違う分布からのデータを利用していいことがあるか、という話。
          たとえば、「DB2をIBM製品として初めて買ってくれる人(Y)」を当てたいんだけど、訓練データがあまりない。一方、「DB2をすでに買ってくてれる人(Y')」のデータはもっとある、と。
          つまりP(X)はおなじなんだけど、P(Y|X)とP(Y'|X)はちょっと違います、と。 で、結論は、P(Y'|X)からのデータを使うことで、varianceが減るけど、biasは増えます、と。
        </UL>
      </UL>
      <LI>β-likelihood
      <UL>
        <LI>今回の<A href="http://www.murata.elec.waseda.ac.jp/yu.fujimoto/">日本人1/3</A>。 となりのポスターでした。
        <LI>解く問題： 混合分布推定
        <LI>アプローチ： β-likelihoodを使う
        <UL>
          <LI>対数尤度のかわりにβ-likelihoodというのをつかえばロバストになります、というのを、今回は混合分布の推定に使った。
          <LI>背後の理論は僕にはわからないけど、、このβ-likelihood、とりあえず使えば、かなり使えるんじゃないかと。
        もう何にでも使えるんじゃないかと！
        </UL>
      </UL>
      <LI>ちなみに、<A href="http://mi.cs.titech.ac.jp/kurihara/index-ja.html">もう1/3の日本人</A>の方のポスターは、EMのEとMを入れ替えるという話。 最尤推定のところを事後分布の期待値にして、逆に隠れ変数の期待値をとるところをhard
        assignmentにする。 そうすると実装的に都合が良いとのこと。
      <UL>
        <LI>そういえば、2年前の日記にかいた<A href="research_diary-2004.html#20040930">PCFGのVB推定</A>の論文、この方のでした。 このときの疑問をきいとけばよかった。
      </UL>
    </UL>
    <LI>キーノート
    <UL>
      <LI>プライバシな話
      <UL>
        <LI>内容は<A href="http://www.google.co.jp/search?sourceid=navclient&hl=ja&ie=UTF-8&rls=GGLG,GGLG:2006-04,GGLG:ja&q=Privacy%2DEnhanced+Linking">コレ</A>
        <LI>どうも講演者はk-anonymityとかいうプライバシの評価手法を提案したひとらしい。この情報で特定される人がk人以上ですよ、みたいなかんじかな。
        <LI>話の内容は、リンク解析アルゴリズムの設計者は、自らのリンク解析アルゴリズムに責任をもってプライバシーをまもる仕組みをいれなさい、みたいな話。
        <UL>
          <LI>ここではリンク解析とは、law-enforcement とか counter terrorismとかを目指して、分断されたデータから知識を得る技。分散データマイニングとかいうより、監視システムとかで、いろんな情報から個人を特定してトラッキングするという意味で使っているぽい。
        </UL>
        <LI>そういえば、プライバシに関する要請って、使用目的とか、何が記録されたのか、漏れないのが保証されているとか、っいうのが主なんだろな、って思ってたんだけど、ほかにも「集められた個人情報は正しくなければいけない」という要請もある。 言われてみればそうなんだけど、ああ、気にしたこと無かったなあてかんじ。
      </UL>
      <LI>ネットワークな話。
      <UL>
        <LI>度数数えたり、ノードの属性をつかわないgenerative modelだけなのかとおもってたら、ちゃんとMRF的なモデルも研究されているようです。
      </UL>
    </UL>
    <LI>ワークショップ
    <UL>
      <LI>scalable data miningみたいなワークショップに座る（すわるだけ）。
      <UL>
        <LI>グリッドでマイニングしますな話
        <UL>
          <LI>remotely executable WeKaをやってみました。
          <LI>データ送信とかは全体にくらべるとたいしたオーバーヘッドにはなりません
        </UL>
        <LI>新しいチップのアーキテクチャ（マルチコアみたいなやつ？）にマッチしたマイニング
        <UL>
          <LI>著者の半分はインテルのひとが入っているので、たぶんそれなりに本気
          <LI>グラフマイニングのアルゴリズムを並列化してくらべてみました。 gSpan, GASTON,
          MeFa,FFSM を比べて速いのは1位GASTON2位gSpanだったけど、GASTONはメモリ喰いすぎらしい。
          <UL>
            <LI>ここで、Han先生、「それは我々の実験ともconsistentだ。 他の2つはgSpanより速いって主張してるけど、それはありえん」とご満悦
          </UL>
        </UL>
        <LI>Han先生ご講演：RFIDデータの解析
        <UL>
          <LI>RFIDのデータを解析するのに、どういうデータベースをつくったらいいでしょう、という話。
          ベースはICDE06のbest student paper awardをとった話とのこと
          <LI>1個1個のRFIDをトラッキングするデータをもっとくのではなく、まとめて運ぶ単位に抽象化した仮想的なデータをつくって、紐つけとくと便利だよ、みたいな話。
        </UL>
      </UL>
    </UL>
    <LI>あと、どうでもいい感想
    <UL>
      <LI>行きの飛行機でインスタントなんとかヌードルみたいのがでたのだが、入れてくれるお湯が人肌？てくらいぬるい。
      こんなんで喰えるか！とか思いながら、完食。 うどんのスープ＋ラーメンの麺。インスタント沖縄そばとは、ちょうど逆。
      <LI>2日目の朝ごはんはベーグルだった。ベーグル萌えなので、2個食べる。 基本的に毎朝コーヒー＋小麦製品1品。コンチネンタルって、手抜きのこと？
    </UL>
  </UL>
  <LI><A name="20060418"></A>2006/4/18 とりあえず、くんたまをカバンにいれる
  <UL>
    <LI>あしたから腸炎片手（まじ、はらいたい）に、<A href="http://www.siam.org/meetings/sdm06/">SDM</A>発表アンド、ワトソン詣でにいってくる。
    <LI>SDM、プログラムを見ても著者書いてない。
  </UL>
  <LI><A name="20060412"></A>2006/4/12 <A href="http://people.csail.mit.edu/nati/Publications/thesis.pdf">行列の分解 Y=UV を学習の観点からみる</A>
  <UL>
    <LI>n×m行列Yを、n×k行列Uと k×m行列Vの積に分解するような話は、線形代数で習うような話であるが、この問題をUとVを学習する（＝損失関数を定義して、最適化する）という観点からみると、面白い性質がみえてくる。
    <LI>この問題は、なんらかの損失関数 Loss(Y-UV) を最小化するようなUとVを、ランクがk以下の行列から探すような問題として考えることができる。
    <LI>損失関数が2乗誤差 Loss（Y-UV）=||Y-UV||<SUB>Fro</SUB> （||・||<SUB>Fro</SUB>＝フロベニウスノルム、要するに各要素の２乗の和）のとき、局所解＝大局解である。
    ただし、この問題は凸ではない。 要するに、凸じゃないんだけど、山は１つ、というギリギリの実はこれは結構特殊な問題らしい。
    <UL>
      <LI>UやVの要素が直交するという制約のもとで、その解は特異値分解（SVD）によって一発で求まる。
    </UL>
    <LI>ここで目的関数を、指数分布族の対数尤度やら、hinge lossやらに変えてしまうと、局所解≠大局解となってしまう。
    <LI>2乗誤差を損失関数にする場合でも、行列の穴埋め問題になると、ダメになる。
    <UL>
      <LI>穴埋め問題では、Yの要素は部分的にしか分かっていない。 したがって、損失Loss（Y-UV）も、Yのわかっている要素に対応したところの損失だけを使って定義される（「各要素の損失に重みをつける」のの特殊な場合）。
      穴埋め（予測）は、求まったUとVを使って、UVの対応する要素によって与えられる。
    </UL>
    <LI>さて、どうやらランクの制約をいれると、あまり問題の性質がよくないことがわかった。
    では、制約をUとVの「ランクを制約する」にするのではなく、UとVの「ノルムを制約する」に変えるとどうなるか。
    なんとこれは凸な制約になる。 つまり、目的関数も凸なら、凸な最適化問題になる。
    これは都合がよい。 そして、穴埋め問題になっても、この凸性は崩れない。
    <UL>
      <LI>ノルムを制約する問題は半正定値計画（SDP）として書くことができる。
    </UL>
    <LI>Yの要素Y<SUB>ij</SUB>が+1か-1で、Y<SUB>ij</SUB>(UV)<SUB>ij </SUB>≧ 1 といういわゆるマージンの制約を入れてノルムを最小化しようとするのが、<A href="http://people.csail.mit.edu/nati/Publications/MMMFnips04.pdf">max-margin matrix factorization</A>である。
  </UL>
  <LI><A name="20060407"></A>2006/4/7 入学
  <UL>
    <LI>今月から博士課程に。 <A href="http://www.bic.kyoto-u.ac.jp/takutsu/index_J.html">阿久津先生のところ</A>にお世話になることになる。
    <UL>
      <LI>そもそも、<A href="http://www.kyoritsu-pub.co.jp/bit/soumokuji/index1999.html">bit</A>に載っていた記事を読んでネットワークな話に、ひいては機械学習自体にも興味をもつきっかけになっただけでなく、実質デビュー作である、木のカーネルの話も、先生の講義ノートを読んでて思いついたものでもあるしで、そんな人に縁あって師匠になってもらえるというのは激しくありがたい話である。
    </UL>
    <LI>抱負としては、あらゆる学割を検討していこうと思っています。
  </UL>
  <LI><A name="20060406"></A>2006/4/6 ↓なーんて思ってたら、単に、y' = y + log a にして放り込むだけじゃん。 あほか…。
  <LI><A name="20060404"></A>2006/4/4 混合分布の対数尤度を計算するときに必要そうな、 log( e<SUP>x</SUP> + a e<SUP>y</SUP> ) の計算 （僕にプログラミングの話題を期待しているひとはたぶん居ないけれどもね）
  <UL>
    <LI>log( e<SUP>x</SUP> + e<SUP>y</SUP> ) の計算法については、<A href="http://chasen.org/~taku/blog/archives/2004/09/logsum.html">工藤さんの日記にソースが紹介されている</A>。
    <UL>
      <LI>基本的な考え方は log( e<SUP>x</SUP> + e<SUP>y</SUP> ) = max(x,y) + log( e<SUP>min(x,y)-max(x,y)</SUP> + 1 ) と変形することで、eの肩に極端な値が入るのを避ける、というもの。
    </UL>
    <LI>これを、log( e<SUP>x</SUP> + a e<SUP>y</SUP> ) の計算にしたい場合、変えるべき点は、以下の3点、たぶん。
    <UL>
      <LI>初期値の場合： <B>log(a)</B> + y
      <LI>x=yの場合： <B>log (1+a)</B> + y
      <LI>それ以外： max(x,y) + log( e<SUP>min(x,y)-max(x,y)</SUP> + <B>a</B> )
    </UL>
    <LI>「logSumExpなんて他人事さハハン」なんて思ってたのだが、そうでもなかった。
  </UL>
  <LI><A name="20060327"></A>2006/3/27 オンライン異常検出のための、動的でオンラインなquantile estimation
  <UL>
    <LI>異常検出には、いくつかのアプローチがあるが、そのなかのひとつ代表的なアプローチに、確率モデルを用いたものがある。
    複雑で離散的な構造をもった確率モデルを用いて、オンラインでモデル推定と異常検出を行うためには、モデルの動的変化を考慮にいれた、オンラインquantile
    estimationの方法が必要になる。
    <UL>
      <LI>異常検出とは、たとえば、 x<SUB>1</SUB>, x<SUB>2</SUB>, ..., x<SUB>t</SUB>, ... という感じで毎秒ひとつづつデータが到着するとする。 この系列を観察していて、なにか「異常な」データが観測されたときに、「ハイ」っと手を挙げるような話。
      <LI>c-quantileは確率変数xの確率密度分布P(x)の大きいほうから累積して100ｃ％の点x<SUB>c</SUB>のこと
    </UL>
    <LI>確率モデルを用いたアプローチでは、異常っぷりの定義するのに、まず、データの従う確率分布
    P(x) があると考え、これを現在までの（正常と思われる）データから推定する。
    そして、推定されたモデルのもとで、新しく観測されたデータ x<SUB>t</SUB> が「ありえない！」と思ったときに異常と判断する。 では、どこからが「ありえない！」のだろうか。
    <LI>「ありえない！」の定義としては、「P(x<SUB>t</SUB>)がある閾値以下の場合に異常だ」という検出方法がよさそうだろう。 じゃあ、その閾値はどうやって決めるのか。
    それには、おこりうるxを確率の大きいほうから並べて、累積確率があるパラメータ0&lt;c&lt;1を初めて超える点
    x<SUB>c</SUB> を見つけ、P(x<SUB>c</SUB>)以下の確率でおこる x が現れたとき、それを異常とみなせばよいだろう。
    <UL>
      <LI>例えば、P(x)が正規分布の場合、0&lt;c&lt;1に対して、∫<SUB>xc1&lt;x&lt;∞</SUB> P(x) dx = (1-c)/2 、∫<SUB>-∞&lt;x&lt;xc2</SUB> P(x) dx = (1-c)/2 となる点 x<SUB>c1</SUB>とx<SUB>c2</SUB> を求めておき、新たなデータx' について「if x'&gt;x<SUB>c1</SUB> OR x'&lt;x<SUB>c2</SUB> then 異常」というルールで異常を検出する。
    </UL>
    <LI>しかしながら、P(x)が複雑な分布（特に離散データ）だったらどうだろうか。
    例えばP(x)がHMMやBayesian networkなどの、グラフィカルモデルだったりする場合、あるいはもっと簡単にマルコフモデルでもいいし、これらの混合分布でもいい。
    こんなときには、「おこりうるxを確率の大きいほうから並べる」操作は容易ではない。
    <UL>
      <LI>P(x)が正規分布のときには、実はP(x)の大小と、xの大小に一定の関係があった、つまり、xの大きいほうでは
    x'&gt;x &lt;=&gt; P(x') &lt; P(x) という関係があったし、逆に、xの小さいほうでは
    x'&lt;x &lt;=&gt; P(x') &lt; P(x) という関係があったので、簡単に閾値を設定できたが、複雑なモデルでは、このような順序は自明ではない。
      <LI>単純に適当な閾値を設けて、P(x')がこの閾値以下だったら異常、というのでもいいだろうけど、
      それだと、結局全体のうちどのくらいを異常だと思うかというのが設定できないので困るだろう。
    </UL>
    <LI>こういうときには、x<SUB>c</SUB> をなんらかの方法によって求めておく必要がある。 普通に考えると、この x<SUB>c</SUB> は P(x)の P(x)からのサンプリングか、訓練データの順序統計量を求めることによって推定することになるだろう。
    <LI>さて、オンライン異常検出をやる場合、リアルタイムに異常検出を行うため、quantile
    estimationもオンラインで行う必要があるとともに、オンラインでモデルを推定（アップデート）するので、それにしたがってモデルのquantileも動く。
    これを考慮して、quantileを推定してやる必要がある。
    <LI>まわりくどくなったが、それをやるのが、<A href="http://citeseer.ist.psu.edu/chen00incremental.html">この論文</A>。
    <UL>
      <LI>なんか微妙にアドホックな感じもするけど、性能いいみたいだし、実用的にはいいんじゃない？
    </UL>
    <LI>もうちょっと最近になると、<A href="http://www.cse.unsw.edu.au/~lxue/quant.ps">誤差の理論的評価があって、かつ、最近のデータのみに対するonline quantile
    estimation</A>があるようだが、僕にはわからん。
  </UL>
  <LI><A name="20060315"></A>2006/3/15 「<A href="http://jibun.atmarkit.co.jp/lcareer01/rensai/cas003/cas001.html">「ググる」の精度を高めるために必要なもの</A>」
  <UL>
    <LI><A href="http://chasen.org/~taku/">工藤さん</A>のインタビュー。 かなりいいかんじです。
    <LI>「ひとたび何かに取り組むと一気に仕上げてしまう能力と集中力」ほしい。
  </UL>
  <LI><A name="20060313"></A>2006/3/13 <A href="http://nlp2006.r.dl.itc.u-tokyo.ac.jp/#tutorial">言語処理学会のチュートリアル</A>
  <UL>
    <LI>なんといっても、「Topicに基づく統計的言語モデルの最前線 -- PLSIからHDPまで
    --」に激しく満足。まさに、チュートリアルの理想をいっていた。 こうありたい。
    <UL>
      <LI>話の内容は、「多項分布の混合分布のスゴイやつ」。
      <LI>資料は、<A href="http://www.milab.is.tsukuba.ac.jp/~myama/pdf/topic2006.pdf">ココ</A>に置いてあるが、それよりも（というと失礼だけど、とにかく、それほど）、講師の方の話がすばらしかった。
      通常、そのテーマに携わっているぶん、ついつい入り込んでしまいがちな、そして誰にも伝わらない詳細を、大胆に（話の）近似して「メッセージ」が伝わっていた。
      <UL>
        <LI>話すときには、表層的にでもなるべく多くの人に何かが分かった気にさせるのが重要だなあ。
      </UL>
    </UL>
    <LI>我々の「言語処理における識別モデルの発展 -- HMMからCRFまで --」は、最後の工藤さんに救われたかんじ。
    <UL>
      <LI>前半は、細かい計算に入り込んでいながら、そこを誰にも伝えられなかった。
      <LI>工藤さんの実例は、実体験に基づく知見が染み出ていてよかった。
    </UL>
    <LI>ところで、質問ででていた「識別モデルが生成モデルよりイイという証拠を見せろ」、その場では結局答えが出せなかったのだが、実験的にはさておき、理論的には「識別モデルは学習理論によって理論的性能について語れる、つまりデータを集めてくれば、識別性能がよくなることが保証される」ということではないだろうか。
    一方、生成モデルには識別性能に関しては保証がない（もちろん、「データが無限にある極限」では、ただしい生成モデルをつかって、識別もできるんだろうが。）ということかなー。
  </UL>
  <LI><A name="20060308"></A>2006/3/8 情報処理学会の全国大会
  <UL>
    <LI>座長のお仕事で行く。
    <UL>
      <LI>やったことなかったので、なんとなく、こんなん言ってたかなー、という感じで。
      <LI>２セッション連続だったので、午後が丸々もってかれて、結構疲れた。
    </UL>
    <LI>何人か、知り合いや、もと同僚の方とかに会えてよかった。
    <LI>さっぱり分野ちがいだが、セキュリティ関連でひとつ、面白いな、とおもた話「墨塗り」。
    <LI>墨塗りというのは、原文書ちゅうの、たとえばプライバシーに関わる部分を黒くぬって、公開するような話。
    <UL>
      <LI>「至急、１０００万円、ウサ山さんに振り込んどいてね」という証拠文書を公開するときに、プライバシーに触れる部分があるのでそこを塗りつぶして「至急、１０００万円、■■■■■さんに振り込んどいてね」とすること。
    </UL>
    <LI>これを電子署名された原文書に対してそのままやってしまうと、改ざんとみなされてしまうので、そこをうまく解決しましょう、という話。
    <UL>
      <LI>実現はハッシュをうまく使ってやる。
    </UL>
    <LI>発表してたのは、企業の方だったのだが、ほかの研究グループとして挙げられていたのも別の企業だったので、実用的な技術なのだろう。
    政府系の需要が高いとのこと。
  </UL>
  <LI><A name="20060227"></A>2006/2/27 ネットワーク上での教師アリ学習
  <UL>
    <LI>最近、ネットワーク（＝グラフ）の「上」での教師アリ学習を調べていたので、ちょっとまとめてみる。
    <LI>ここでいうネットワークとは、1つ1つのノードが1つのオブジェクト（＝興味の対象）で、グラフ構造（の辺）によって、オブジェクト間の関係が表現されているようなもの。
    ネットワークはノードの集合Vと辺の集合Eから構成されている。 また、各ノードv∈Vには、その特徴を並べた「特徴
    ベクトル」x(v)が与えられている。
    <UL>
      <LI>たとえば、タンパク質のネットワークでは、ノードは1つのタンパク質を、グラフ構造はそれらの間の相互作用関係などをあらわす。
      また、特徴ベクトルは各タンパク質のアミノ酸配列や発現情報などから作られる。
      <LI>あるいは、mixiみたいな、ソーシャルネットワークの場合、ノードは1人の人を、グラフ構造は人の間のお友達関係をあらわす。特徴ベクトルは、各人のプロフィールとかから作られる。
      <LI>ほかにもWeb（ノード：ページ、辺：ハイパーリンクとか、特徴ベクトル：ページの中身）やら、プロセス間の依存関係（ノード：プロセス、辺：依存、特徴ベクトル：プロセス名やら）などいろいろあるだろう。
    </UL>
    <LI>さて、ネットワーク（＝グラフ）の「上」での教師アリ学習を考えるとき、すくなくとも2つの問題が考えられている。
    ひとつは、ネットワークの「ノードを分類する」問題、もうひとつは、「ネットワークを作る」問題である。
    <LI>前者の「ノードを分類する」問題は、グラフ上のいくつかのノードについてラベルが与えられると、それを手がかりに、残りのノードのラベルを予測するような問題である。
    <UL>
      <LI>ちなみに、ネットワーク構造と特徴ベクトルは使っていい。 （特徴ベクトルは使う場合と、使わない場合がある）
      <LI>いまのところ広く受け入れられていると思われる、アプローチは基本的に「となりあう（＝辺でつながれた）ノード同士のラベルは、たぶん一緒だよね」的な考え方に基づいている。 これを難しくいうと、2006/1/31で触れた「<A href="http://www.cs.uchicago.edu/research/publications/techreports/TR-2004-06">manifold regularization</A>」になる。
      <UL>
        <LI>つまり、「君、乱暴者のAくんと友達なの？ じゃあ、君も乱暴者だな？」という推論。
        <LI>特徴ベクトルを入力とする関数 f(x(v)) が f(x(v))&gt;0 のときラベル1、そうでないときラベル-1とするような、2値分類問題の場合、単に訓練データに対する正解率を上げるだけではなく、ノードvとv'が隣り合っているときに
        (f(x(v))-f(x(v')))<SUP>2</SUP> を小さくするような制約（正則化項）をいれてf を学習するような感じになる。
        <LI>ノード間の関係をグラフじゃなくて、<A href="http://www.cbrc.jp/~tsuda/pdf/icml05.pdf">ハイパーグラフ（グループ情報が与えられる感じ。1つのグループに属する人がhyper
        edgeで結ばれる）であらわしたような場合</A>もある。
        <LI>特徴ベクトルと、ネットワーク構造を区別せずに、ネットワーク構造からdiffusion
        kernelを作って、すべての情報をいったん全部カーネル関数にしてしまってから、それと、特徴ベクトルから作ったカーネル関数との線形結合によって全体のカーネル関数を定義して、その係数を学習する、という<A href="http://noble.gs.washington.edu/papers/lanckriet_kernel.html">アプローチ</A>もある。
      </UL>
    </UL>
    <LI>後者の「ネットワークを作る」問題は、ネットワークの一部分（＝部分グラフ）がわかっているときに、グラフの残りの部分を復元する（＝辺を張る）問題になる。
    <UL>
      <LI>これは、2つのノードペアを分類（2つのノードの間に辺があるべきか、そうでないかの2値分類）するような問題として考えることもできる。
      <UL>
        <LI>「間に辺があるノードペア」と「間に辺が無いノードペア」の集合が与えられると、それらを手がかりにして、「間に辺があるかどうかわからないノードペア」に辺があるかどうかを予測することになる。
        <UL>
          <LI>問題によっては「間に辺が無いノードペア」は与えられていないような設定もある。
        </UL>
      </UL>
      <LI>この問題に対するもっともストレートで、理にかなったアプローチは、ノードペアに対応する<A href="http://www.cs.colostate.edu/~asa/papers/sppii.pdf">特徴ベクトルのペアを分類する</A>ような感じだろう。
      <UL>
        <LI>つまり、「どうやら水瓶座と蠍座は相性がいいようだ。 むむ、君は蠍座だな？
          じゃあ、水瓶座のタカシくんとコンビを組みなさい」という推論。
        <LI>実はこのやり方は、（ネットワークを作ることを明示的に意識していたかどうかは不明だが、）<A href="http://citeseer.ist.psu.edu/oyama04using.html">すでにここで提案されていたり</A>する。
      </UL>
      <LI>ノードペアの分類ではなく、あくまで、それぞれのノードのもつ情報にもとづくアプローチとしては、↑で触れたmanifold
      regularizationと同じようなやり方で、ノードvとv'が隣り合っているときに (f(x(v))-f(x(v')))<SUP>2</SUP> を小さく、逆に、ノードvとv'が隣り合っていないときには (f(x(v))-f(x(v')))<SUP>2</SUP> が大きくなるような目的関数を最適化する f を学習するような<A href="http://web.kuicr.kyoto-u.ac.jp/~yoshi/paper/vert_nips04.pdf">アプローチ</A>もある。
      <UL>
        <LI>f(x(v))とf(x(v'))の距離ではなく、相関を最適化する、というふうにすると<A href="http://scholar.google.com/scholar?hl=en&lr=&q=genomic+data+supervised+approach">コレ</A>。
      </UL>
      <LI>さらに、これらのアプローチのどちらをとるかとは別に、また違った観点からの情報を活用するという切り口としては、「ネットワークがスケールフリーになる」、すなわち、ノードの次数がべき乗分布に従う（実際、世の中の多くのネットワークが従う）、という制約を入れてネットワーク構造を予測するという<A href="http://www.genetics.org/cgi/content/full/159/3/1291">アプローチ</A>もある。
      <UL>
        <LI>最近、「これも、あれも、スケールフリーですよ」みたいな話があふれているが（それはそれで「へええ」なのではあるが）、役に立つ使い方をしているものはほとんど無い。
        <LI>この研究はスケールフリー性という性質を、具体的に予測精度の向上に結びつけようという試みとして、重要であるとおもう。
        <UL>
          <LI><FONT size="-1">（ていうか最近、このアイデアを思いついて暫く考えていたのだが、この論文の存在を阿久津先生に教えてもらってかなりションボリへこむ…。）</FONT>
        </UL>
        <LI>グラフ上の学習とは別文脈だが、ベイジアンネットワークの構造推定で、事前分布としてべき乗分布を使うという<A href="http://www.is.titech.ac.jp/~shimo/pub/GIW2005/GIW05P068.pdf">試み</A>も始まっているような模様。<FONT size="-1">（こっちは上田さんに教えてもらいました。）</FONT>
      </UL>
    </UL>
  </UL>
  <LI><A name="20060226"></A>2006/2/26 <A href="http://www.siam.org/meetings/sdm06/">SIAM Data Mning Conference</A>
  <UL>
    <LI><A href="http://www.siam.org/meetings/sdm06/program.htm">program</A>が出ている。 面白そうなものがいくつかあるもよう。
  </UL>
  <LI><A name="20060215"></A>2006/2/15 みつけたもの
  <UL>
    <LI><A href="http://www.ibm.com/investor/viewpoint/podcast.phtml">IBM podcast</A>
    <UL>
      <LI>毎回、IBM内外のエライ人がでてきて、なにか喋る。
      <LI>リスニングの練習にいいかも。
    </UL>
    <LI><A href="http://www.ar.sanken.osaka-u.ac.jp/rm/rm06.html">Workshop on Risk Mining</A>
    <UL>
      <LI>人工知能学会の全国大会に併設の国際ワークショップらしい。
      <LI><A href="http://www.ar.sanken.osaka-u.ac.jp/rm/rm06.html#topics">トピックス</A>的に興味が重なるので見てみたいかも。
    </UL>
  </UL>
  <LI><A name="20060207"></A>2006/2/7 von Neumann Entropy
  <UL>
    <LI>フォン・ノイマン・エントロピーは、半正定値行列Kに対して、-tr(KlogK)と定義される。
    ただし、tr(K)=1
    <UL>
      <LI>tr(K)は、Kの対角成分の和 = Kのすべての固有値の和
    </UL>
    <LI>フォン・ノイマン・エントロピーは、普通の（Shannonの）エントロピーを行列に拡張したものだとして解釈できるらしい。
    <UL>
      <LI>ちなみに普通のエントロピーは確率分布p(x)について、-Σ<SUB>x</SUB> p(x) log p(x) で定義される。
    </UL>
    <LI>具体的には、Kの固有値についてのエントロピーだと解釈できる。
    <LI>まず、以下の2つの制約から、Kの固有値が確率分布をつくっていることがわかる。
    <UL>
      <LI>Kが半正定行列 ⇒ Kのすべての固有値は0以上である。
      <LI>tr(K)=1 ⇒ tr(K)はKのすべての固有値の和に等しいので、tr(K)=1であるということは、Kのすべての固有値の和が1になる。
    </UL>
    <LI>von Neumann Entropy -tr(KlogK) はこの確率分布のShannonエントロピーになっていることがわかる。
    <UL>
      <LI>準備： K=A<SUP>T</SUP>DAのように分解する。 ここでAは直行行列、Dは対角成分に固有値d<SUB>1</SUB>,d<SUB>2</SUB>, ...がならんだ対角行列
      <LI>「固有値の絶対値が1より小さい行列の対数は log (I+X) = Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> X<SUP>n</SUP>、 ここで、α<SUB>n</SUB>= (-1)<SUP>n+1</SUP>/ n 」より…
      <UL>
        <LI>X = K - I とおくと、log(K) = Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> (K-I)<SUP>n</SUP>
        <UL>
          <LI>変形を続けると、log(K) = Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> (A<SUP>T</SUP>DA - A<SUP>T</SUP>IA)<SUP>n</SUP> = A<SUP>T</SUP> ( Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> (D - I)<SUP>n</SUP> ) A
          <LI>よって、K log K = A<SUP>T</SUP> D ( Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> (D - I)<SUP>n</SUP> ) A
        </UL>
        <LI>X = D - I とおくと、log(D) = Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> (D-I)<SUP>n</SUP>
        <UL>
          <LI>よって、D log D = D Σ<SUB>n=1〜∞</SUB> α<SUB>n</SUB> (D-I)<SUP>n</SUP>
        </UL>
      </UL>
      <LI>従って、K log(K)  = A<SUP>T</SUP> D log(D) A、直行行列がかかっていてもtrの値は変わらないので、-tr{ K log K }= -tr{ D log D }
      <LI>- tr(D log D) = - Σ di log di なので、固有値のエントロピーになる。
    </UL>
    <LI>何に使うか？ 分布の学習をするときに、もっとも滑らかな分布を求めるのに、制約を満たしながら分布のエントロピーが最大になるように学習を行うという考え方があるが、これと同じように、制約を満たしながらもっとも滑らかな「行列」を学習するのに使える。
    <UL>
      <LI>応用については、たとえば<A href="http://noble.gs.washington.edu/papers/tsuda_learning.pdf">カーネル行列を学習する話</A>などがある<FONT size="-1">（ていうか、これを読んでて知ったんだけど…）</FONT>、これについてはまた後日…。
    </UL>
  </UL>
  <LI><A name="20060131"></A>2006/1/31 <A href="http://people.cs.uchicago.edu/~vikass/TR-2004-06.pdf">manifold regularizationによる半教師アリ学習</A>
  <UL>
    <LI>半教師あり学習の一手法、すなわち、通常の分類問題において、訓練データ（入力XとクラスYの組）に加えて、Yが与えられない（Xだけの）データが与えられているときに、これらのデータをいかに活用するか、という話。
    <UL>
      <LI>クラスが与えられてないデータはP(Y|X)を推定するのには、直接には役にたたなさそうだが、P(X)についての情報はもっているから、これをP(Y|X)の推定に役立てよう、というのが狙い。
    </UL>
    <LI>モデル<FONT size="-1">（ここでは、分類問題を考えるので、入力XからクラスYへの予測分布P(Y|X)とする）</FONT>を訓練データ（入力XとクラスYの組）にフィットさせるとき、たいていの場合、正則化（regularization）ということが行われる。
    <UL>
      <LI>正則化というのは要するに、予測分布になんらかの「滑らかさ」を仮定することで、訓練データからの一般化能力を高めることを意図して行われる。
      <LI>これは、モデルのデータへの当てはまりを評価する項（尤度とか）に加えて、正則化項として、パラメータの２乗ノルムなどの、予測分布の複雑さ（＝滑らかでなさ）を制限するペナルティ項をいれることによって実装される。
    </UL>
    <LI>Manifold regularizationでは、正則化のひとつとして、「P(Y|X)がP(X)について滑らかになる」ような制約を入れる。
    <UL>
      <LI>つまり、データが疎なところでは思いっきり滑らかに、密な所は少々複雑になってもいい、という感じの、P(X)の濃さに応じた形での滑らかさを強制する。
    </UL>
    <LI>実際のところ、P(X)はわからないので、クラスなしデータも含めた訓練データを利用して行う。
    <UL>
      <LI>しかしここではP(X)を推定して…ということはやらずに、データ同士の隣接関係を表した（たとえばk-nearest neighbour同士を辺で結んだ）グラフを用いる。
      <UL>
        <LI>入力空間における近傍関係を表したグラフが、ここでいう「manifold」の近似になる。
      </UL>
      <LI>入力空間を表したグラフ上で隣接するデータにおいてはP(Y|X)が近い、つまりこれらの間でのP(Y|X)の差に対してペナルティを与える、というように実装している。
    </UL>
    <LI>文脈は違うけど、最近やりたかったことがある意味実現されてしまっていた、が、もうちょっと別の方向から考えてみよう…。
  </UL>
  <LI><A name="20060125"></A>2006/1/25 particle filter
  <UL>
    <LI><A href="http://ibisforest.blog4.fc2.com/blog-entry-25.html">IBISブログでも「わかりやすい」と紹介されていた</A>が、以前から気になってたparticle filterの紹介記事が<A href="http://www.ieice.org/jpn/books/kaishikiji/2005/200512.pdf">ココ</A>でよめる（正規のルートで）ので読んでみた。 確かに分かりやすい解説だとおもう。
    オススメ。
    <LI>結局何をやってくれるのがparticle filterか？というと <FONT size="-1">（正しい流れとは違うんだろうけど、僕を含む）</FONT>そもそもHMMを一番初めに知った人へのイメージでいうと、HMMを連続値にしましたよ、で、遷移確率も出力確率もなんか難しそうな分布になりましたよ、と。 そうしたら、尤度の計算とかできなくなっちゃったんで、サンプリングでやりましたよ、という感じ。 たぶん。
    <LI>後半、フィルタリングのところがGAでいう淘汰と同じように解釈できるとか、GAとの類似性が語られるが、GAもこういう文脈で出されると、ちょっと面白い。
  </UL>
  <LI><A name="20060120"></A>2006/1/20 JMLR
  <UL>
    <LI>リスクのやつの<A href="publication/risk_submitted.pdf">full paper版</A>を、とりあえず、そのままで出してみた。
    <LI>まあ、そのままでOKとかには、ならんのだろな…。
    <LI>（← あとでみてみたら、エディタが<A href="http://www-cse.ucsd.edu/~elkan/">Charles Elkan</A>になっていた！（彼はcost-sensitive learningの草分け）。 結果はどうあれ、僕のこの話が彼の目に触れるのは重要。）
  </UL>
  <LI><A name="20060117"></A>2006/1/17 <A href="http://www.dumbo.ai.kyutech.ac.jp/hirata/TreeKernel/index.htm">九州で木カーネル</A>
  <UL>
    <LI>週末に九州のセミナーに行ってきた。
    <UL>
      <LI>「木とカーネル」という名前は、「木のカーネル」じゃなくて、「木」の部と「カーネル」の部に分かれていたらしい…。
    </UL>
    <LI>畳み込みカーネルは基本的に、２つの構造の共通部分構造を数え上げる問題になるが、僕の話は、もともと、順序木のカーネルを、部分グラフを部分構造として作る話。
    <LI>まず1つ目の拡張方向として、<A href="http://www.donald.ai.kyutech.ac.jp/~hiroshi/">坂本さん</A>の話は、順序なしの一般の木構造のカーネルを、同じく部分グラフを部分構造とすると、計算できなくなる、という話。 木に色々な数のダミーノードをつけて、連立一次方程式にもっていく還元がかっこいい。
    <LI>じゃあ、一般の木の場合には、どうするの？っていうので、木の構造の特徴を捉えて、かつ、計算が簡単な部分構造が必要になる。
    <UL>
      <LI><A href="http://www.dumbo.ai.kyutech.ac.jp/hirata/">平田先生</A>や<A href="http://www.ccr.u-tokyo.ac.jp/~ori/">久保山先生</A>の木のq-gramというのは、パスと同型な部分構造を使っている。 うまくやると、線形時間で計算できる。
      <LI>他のアプローチとして、正直に部分グラフを使うが、計算を近似にする、というのもある。
    木の世界ではこの手のことをやるいろいろな近似アルゴリズムがあるようです。
    </UL>
    <LI>もう１つの拡張方向として、順序木の場合の共通部分構造の数え方に曖昧性をもたせる（＝ちょっとだけ違う部分構造も同じとみなす）、という方向で、久保山先生の話は、この曖昧性というのを、木写像の言葉で明確にクラス分けしたもの。
    <UL>
      <LI>この分類では、もともと僕がやったのは、位相保存写像というわりと限定された写像のクラスになるようだ。
      <UL>
        <LI>片方の部分構造を、もう片方の部分構造に埋め込むときに枝を伸ばしても良いが、埋め込みが枝を共有してはいけない。
    １月に出たジャーナルの図では、ここを間違えていた…。
      </UL>
      <LI>もっとも一般的な写像クラスとしては、<A href="http://scholar.google.com/scholar?hl=en&lr=&c2coff=1&q=%22The+tree--to--tree+correction+problem%22">Tai (1979)</A> の提案した写像があるが、これに対しても同様にカーネル計算のアルゴリズムが設計できる。
    </UL>
    <LI>やっぱり、各分野のホントの専門の人は、いろいろ引き出しがある。
  </UL>
  <LI><A name="20060116"></A>2006/1/16 <A href="http://www.geocities.jp/kashi_pong/publication/Risk_SDM.pdf">SDMのカメラレディ版</A>を出した。
  <UL>
    <LI>１２ページから５ページに一気に圧縮。
    <LI>もともとのほうはもったいないのでそのままジャーナルにまわしちゃおうかな…。
  </UL>
  <LI><A name="20060110"></A>2006/1/10 【宣伝】 職場のオープンハウス
  <UL>
    <LI>僕は直接関わっているわけではないですが、職場のオープンハウスがあります。
    申し込みが必要だそうです。
    <UL>
      <LI>2006/1/31 神奈川：<A href="http://www.research.ibm.com/trl/oph06/">TRL Open House 2006</A>
      <LI>2006/2/7 京都：<A href="http://www.research.ibm.com/trl/oph06/kyoto.htm">TRL Open House 2006 in Kyoto</A>
    </UL>
    <LI>職探しの一環にどうぞ。 神奈川であるほうについては、事前に連絡をいただければ、来られたついでの個人的応対もできます。
  </UL>
  <LI><A name="20060106"></A>2006/1/6 今年も開始
  <UL>
    <LI>この日記も4年目に突入。 今年もダラダラ研究生活でいこうと思ってたのだけど、なんだかんだで職場でももう8年目に入る年、なんだかそろそろモラトリアム終了な感じが漂ってきました…。
    <LI>ところで、年末〜正月のオフライン隠遁生活で読んでた本「<A href="http://www.amazon.co.jp/exec/obidos/ASIN/4756908489/">価格決定戦略</A>」
    <LI>今後、経営や様々な企業活動において、（ほどほどに）科学的なアプローチの重要性は高まってくるだろうが、この本は、企業活動の中でもマーケティング分野、特に、プライシング、つまり、商品やサービスの値段をいくらに設定するか、という問題へのアプローチをやさしく解説している。
    <LI>値段づけの問題は、単純に安くすれば、売り上げが多くなる（…で、値段に伴う販売量を予測して、売り上げ最大になる値段を決めるとか）みたいな単純な話ではない。
    安易にセールを行って安くしてしまうことで、消費者が妥当だと感じる金額を下げてしまい、もとの値段に戻ったときに、却って売り上げが落ちてしまい、長期的にはマイナスになってしまうことがあるなど、心理的な要素が大きく関わってくるため単純にはいかない。
    商品の入るカテゴリをうまく設定したりとか、クーポンやバージョン分けだとかによって、うまく値ごろ感を演出するための様々な工夫が紹介されている。
    <LI>この本は数理的、というよりはかなり読み物的に、例も多くて非常に分かりやすく、とにかくおもしろポイント中心で書かれている。
    これまでにもプライシング関連の本は読んでみようと何冊か手にとってはみたものの、中途半端に数理的な本を読もうとして、結局何がやりたいのか分からずに却ってあまり興味が持続しなかったのだが、これは巻頭で著者が述べているようにひたすらにわかりやすさを求めている。
    好感。
    <UL>
      <LI>字が大きいので、そのまま縮小して文庫本サイズにしてくれるともっとありがたかったのだが。
    </UL>
  </UL>
</UL>
<HR>
<P>ちなみに、このサイトの掲載内容は私自身の見解であり、必ずしもIBMの立場、戦略、意見を代表するものではありません</P>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1197929-1";
urchinTracker();
</script>
</BODY>
</HTML>