<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Statistical Learning Theory (2025), Graduate School of Informatics, Kyoto University</title>
<meta http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
<meta content="MSHTML 6.00.6000.17093" name="GENERATOR">
<meta http-equiv="Content-Style-Type" content="text/css"></head>
<body>
<p><font size="+2"><b>Statistical Learning Theory (2025), Graduate School of Informatics, Kyoto University<br>
</b>
</font><br>
Lecturers: <a href="https://hkashima.github.io/index.html">Hisashi Kashima</a>, <a href="https://riken-yamada.github.io/profile.html">Makoto Yamada</a>, and <a href="https://koh-t.github.io/jindex.html">Koh Takeuchi</a><br>
  Day, time, and room: Monday, 8:45-10:15 / Research bldg. 8, Lecture room 2<br>  
<br>
<br>

This course will provide a in-depth exploration of the foundational theory and practical applications of statistical machine learning, which plays a significant role in statistical data analysis and data mining. We will primarily focus on supervised and unsupervised learning, with an emphasis on supervised learning. The course will cover essential theoretical concepts such as maximum likelihood estimation and Bayesian inference, as well as introduce the concept of Probably Approximately Correct (PAC) learning.
Throughout the course, you will gain familiarity with various probabilistic models and predictive algorithms, including logistic regression, perceptrons, and neural networks. Additionally, we will touch upon advanced topics like semi-supervised learning, transfer learning, and sparse modeling, providing you with insights into the latest developments in the field of machine learning. In addition, opportunities for hands-on data analysis exercises will also be provided.

<br>
<br>
<strong>[Course Materials]</strong> <br>
 1. <a href="SML2025_introduction.pdf">Introduction to Machine Learning</a> (Kashima)<br>
 2. <a href="SML2025_regression.pdf">Regression</a> (Kashima)<br>
 3. <a href="SML2025_classification.pdf">Classification</a> (Kashima)<br>
 <!--
 4. <a href="https://groups.oist.jp/mlds/statistical-learning-theory">Feature Selection and Sparsity</a> (Yamada)<br>
 5. <a href="https://groups.oist.jp/mlds/statistical-learning-theory">Dimensionality Reduction</a> (Yamada)<br>
 6. <a href="SML2025_learningtheory.pdf">Statistical Learning Theory</a> (Kashima)<br>
 7. <a href="SML2025_neuralnet.pdf">Neural Networks</a> (Kashima)<br>
 -->
<br>


<strong>[References]</strong><br>
- Lecture slides for the previous years: 
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2014.html">2014</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2015.html">2015</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2016.html">2016</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2017.html">2017</a>, 
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2018.html">2018</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2019.html">2019</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2020.html">2020</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2021.html">2021</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2022.html">2022</a>
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2023.html">2023</a>
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2024.html">2024</a>
  <br>
<br>


</body></html> 
