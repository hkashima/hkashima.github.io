<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> 
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
<META http-equiv="Content-Style-Type" content="text/css">
<TITLE>機械学習についての日々の研究</TITLE>
</HEAD>
<BODY>
<p><a href="https://hkashima.github.io/research_diary.html">もどる</a> 
</p>
<P><FONT size="+2"><B>（か）研究日記</B></FONT></P>
<HR>
<UL>
  </UL>
<UL>
  <LI><A name="20071227"></A>2007/12/27 行列のランクの最小化
  <UL>
    <LI>最近、ランク最小化ものに少しはまっていたので、ちょっとだけまとめ。
    <LI>ある学習問題を考えたときに、求めるべきパラメータが行列の形をしていることがある。
    たとえば、
    <UL>
      <LI>入力が行列Xであるような予測器 f(X) = &lt;W, X&gt; を学習したかったり、
      <LI>行列Xに観測できる部分とそうでない部分があって、この行列の分解 W=UV を考えることによって穴埋めをしたかったり、
    </UL>
    など。 このとき、パラメータWも行列であるため、その次元数は（一辺をDとするとD×Dになるわけだから）非常に大きくなる。
    <LI>そこで、当然のことながら、行列パラメータに対して、なんらかの制約を課する、つまり、正則化を行ったり、事前分布を考えるということを思いつく。
    <UL>
      <LI>まず思いつくのは、フロベニウスノルム（行列の要素の二乗和）を使うということだろう。
      <UL>
        <LI>しかしこれは、行列を延ばしてベクトルにしたものの2乗ノルムを考えているのとおなじだから、「行列ならでは」という感じではない。
      </UL>
    </UL>
    <LI>パラメータが行列であることを意識すると、「行列のランク」を正則化に使うというのは、非常に自然な考え方だろう。
    <UL>
      <LI>一般に行列のランクというのは、行列のある意味での自由度を定義していることで、これを抑えるというのはなかなかリーズナブルな考え方である。
    </UL>
    <LI>しかし、ここで問題が発生する。 行列のランクというのは凸関数ではないのである。
    従って、行列のランクと同じように振舞う、凸関数があるといいだろうなあ、という話になってくる。
    <UL>
      <LI>最近、機械学習でも、行列のランク（を間接的に小さくする凸関数）を正則化に使おうという考え方が流行しているようだ。
    いくつか挙げると、
      <UL>
        <LI><A href="http://ttic.uchicago.edu/~nati/mmmf/">Matrix Factorization</A>
        <LI><A href="http://www.sat.t.u-tokyo.ac.jp/~ryotat/TomAih07.pdf">複数時系列の分類</A>
        <LI><A href="http://www.machinelearning.org/proceedings/icml2007/papers/229.pdf">多クラス分類</A>
      </UL>
      などがある。
      <LI>これらの元を、（ほんのちょっとだけ）辿ると、どうやらFazelの一連の研究（&quot;Convex
    Optimization&quot;のBoydの弟子）にたどり着く。
      <UL>
        <LI>彼女の書き物のなかではこの<A href="http://www.cds.caltech.edu/~maryam/acc04-tutorial.pdf">チュートリアル資料</A>が非常に良くまとまっていて読みやすい。
      </UL>
    </UL>
    <LI>解決策としては、アイディアは、（凸でない）ランクのかわりに、（凸である）行列のトレースを使う。
    <UL>
      <LI>半正定値行列のトレースは、固有値の絶対値の和（つまり、固有値をベクトルと見た場合のL1ノルムに相当する）であるから、これを正則化項として小さくすることは、多くの固有値が0になり、結果的に、行列のランクが下がるのである。
      <UL>
        <LI>つまり、もともと最小化したかった目的関数は 損失関数L(W) + 正則化項 <B>rank</B>(W) だったのが、これを、 損失関数L(W) + 正則化項 <B>Tr</B>(W) で置き換えようというのである。
      </UL>
      <LI>なお、半正定でない場合（2つの半正定値行列の差として表現し、それぞれのトレースを最小化する）や、対称でない場合、正方でない場合（semi-definite
      embeddingというテクニックを使って、（行列は大きくなるけど）対称行列にもっていく）も扱うことができる。
      <LI>適当な損失関数を使えば、最適化問題としては、半正定値計画問題（SDP）などになる。
      <UL>
        <LI>なお、半正定の制約というのは大きい問題では結構きつい。 従って、大きい問題では、固有値のL1ノルムを丸くして微分可能にしたり、凸になるのを諦めて適当に解いたりする。
        <LI>対称行列の場合には、<A href="http://people.csail.mit.edu/jrennie/writing/convexLR.pdf">diagonal dominance theorem</A> （各行の対角成分が、その行の（対角以外の成分の）L1ノルムより大きいならば、半正定）
        を使って、線形制約にする方法も<A href="http://people.csail.mit.edu/romer/papers/RosFun_Sub.pdf">ある</A>。
        <UL>
          <LI>ただし、逆は成立しないので、これは緩い。 例えば、対称でない場合には、前述のsemi-definite
          embedding で対角化してから用いると、結局L1ノルムと同じになってしまう。
        </UL>
      </UL>
    </UL>
    <LI>さらに、別の正則化項として log det(W) を使うことで、Tr(A) よりも、さらにランクが落とせるという話もある。（前述Fazelらによる）
    <UL>
      <LI>log det(W) は凸ではなく、凹なので、現在の解のまわりで線形近似ながら繰り返し解きなおす（最近、機械学習で使われている<A href="research_diary-2006.html#20060803">CCCP (Convex-Concave Procedure)</A> と呼ばれている手法に同じ）ことで、局所解が求まる。
      <LI>ちなみに、この繰り返しの1ステップ目は、ちょうど Tr(A) を使った正則化に一致する。つまり、log
      det(W) を使うことで、Tr(W)よりもさらにランクを落とすことが期待できる。
    </UL>
  </UL>
  <LI><A name="20071224"></A>2007/12/24 年末に大漁（SDM＆PAKDD）
  <UL>
    <LI>かしまが関わっている論文が<A href="http://www.ar.sanken.osaka-u.ac.jp/pakdd2008/">PAKDD</A>に１本、<A href="http://www.siam.org/meetings/sdm08/">SDM</A>に３本アクセプトされました。
    <LI>PAKDDのほうは、我々のお仕事寄り論文で、<A href="http://www.trl.ibm.com/people/hido/index_e.htm">比戸さん</A>ファーストの、変化点分析の話。
    <UL>
      <LI>我々のはlong paperでアクセプトされたのだが、「Only approximately 12% of
      the 312 submissions were accepted as long papers, 13.5% of them were accepted
      as regular papers, and 12% of them were accepted as short papers. 」とあるようにロングとレギュラー、ショートの３段階もあるらしい...。
    </UL>
    <LI>SDMのほうは、なんと３本も...。
    <UL>
      <LI><A href="http://www.cb.k.u-tokyo.ac.jp/asailab/kato/">加藤さん</A>がIBISで発表していた、複数のネットワークを組み合わせたノード分類の話。
      
      <LI><A href="http://www.trl.ibm.com/people/hido/index_e.htm">比戸さん</A>の、偏りのあるデータに対するバギング再考の話。 
      <LI>坪井くんが、杉山さんの密度比推定法（DMSSで発表してたやつ）を、大規模にした話。
      
    </UL>
    <LI>はい、今回も「創造力を刺激するあなたのパートナー」ないし「あ、そこ、○○したらいいんじゃない？」的な貢献です。
    ありがたい！
    <LI>そのほかのTプラの皆様も含めると、P<A href="http://sugiyama-www.cs.titech.ac.jp/T-PRIMAL/publications-jp.html">AKDD２本＋SDM７本の、たいへんな大漁</A>になっております。 スゴイよみんな！
  </UL>
  <LI><A name="20071210"></A>2007/12/10 宣伝： <A href="http://research.microsoft.com/~cmbishop/PRML/">Bishop本</A>の<A href="http://ibisforest.org/index.php?PRML">日本語訳、そろそろでます</A>
  <UL>
    <LI>翻訳プロジェクトに仲間にいれていただいた「Pattern Recognition and Machine
    Learning」の日本語版、上下のうち上巻がそろそろ出るようです。
    <LI>この本は、僕もちゃんと全部読めてはいないですが、機械学習の勉強をするのには、現段階では恐らくベストの本ではないかと思います。
    <LI>上下併せると結構高くなっちゃいますが、ちゃんとこの分野で働いている人たちがまじめに翻訳に取り組んだので、かなり信頼の置ける翻訳になっており、よくある「結局、原書読まないとわからん」ということはほとんどないと思います。
    それなりにもとは取れるのではないでしょうか。
    <UL>
      <LI>神嶌さんの怒涛の取りまとめっぷりによって全体感も保たれております。
      <LI>そして、監訳の大先生方も、ほんとにちゃんとチェックされているのです。
    </UL>
  </UL>
  <LI><A name="20071116"></A>2007/11/16 ICDM、ちょっと微妙だったかもの巻
  <UL>
    <LI><A href="http://www.cse.ust.hk/~qyang/ICDMDMC07/">コンテスト</A>の勝者プレゼンするために、<A href="http://www.ist.unomaha.edu/icdm2007/">ICDM</A>にいってきました。<BR>
    （あ、どうも）＞<IMG src="award.jpg" border="0" width="181" height="192">＜（ほれ）
    <UL>
      <LI>ちなみに、<A href="http://www.geocities.jp/kashi_pong/publication/ICDMDMC2007slide.pdf">label propagationでサクっといったように書いてる</A>けど、それなりには苦労しました。
      <LI><A href="http://www.cse.ust.hk/~qyang/ICDMDMC07/">コンテストのページ</A>で、データセットも公開されたようです。
    </UL>
    <LI>会議全般
    <UL>
      <LI>規模は若干縮小
      <UL>
        <LI>今回のサブミッションは530（去年は900ちかくあった）と、KDDの573よりも少なかった。
        参加者は300ちょっと、やはり減っている。
        <UL>
          <LI>まあ、去年は中国（香港）だったのに対し、今回はアメリカのど真ん中だからというのもあったのかも。
        </UL>
      </UL>
      <LI>仕組みは若干前進
      <UL>
        <LI>今年から査読がダブルブラインドに。
        <UL>
          <LI>分析によると、有力者を著者にいれたときの採択率が、一般のアクセプト率に近いところまで減ったらしい（！）
          <UL>
            <LI>つまり、以前は、一般人のみのチームよりも、偉い人を入れたチームのアクセプト率が高かったということ。
          </UL>
          <LI>なお、日本からのアクセプトは意外に多い、5/24=17%
        </UL>
        <LI>今回からワークショップのproceedingsをちゃんとアーカイブするようにしたとのこと、が、しかし、その弊害か、ワークショップにno
        showだらけ（本会議は、誰か1人は登録する義務あり）
      </UL>
      <LI>そのほか若干微妙
      <UL>
        <LI>バンケットは、ほとんどスポンサー（チェアの上司とか、動物園の園長とか...）のトーク、これが結婚式のスピーチのようで非常にだるい。
        しかも、しゃべってると注意される...。 
        <LI>が、遠足（<A href="http://www.omahazoo.com/">動物園</A>）はよかった。 全米ナンバーワン（？）の動物園らしい。 ほんとか。
      </UL>
    </UL>
    <LI>招待講演、チュートリアルには、ためになるものもあった。
    <UL>
      <LI>テンソル（高次元の行列、ていうか3次元）ものの招待講演チュートリアルは、結構面白かった。
      <UL>
        <LI>例えば、SVDのテンソル版とかの話。
        <UL>
          <LI> 基本的には、2次元のSVDの繰り返しに帰着するような解法のようだ。 たぶん。
        </UL>
        <LI>アプリケーションは、たとえば、
        <UL>
          <LI>キーワードつきのHITS, PageRank。
          <UL>
            <LI>有向グラフの行列表現で2次元、キーワードで1次元。 コレに対して3次元SVDをかます。
          </UL>
          <LI>固有顔（顔画像の低次元近似表現、人と画像で2次元）に輝度の1次元を入れる。
          
        </UL>
        <LI>なお、本会議のほうでも、行列系で、1つセッションがあった。
        <UL>
          <LI>ランダムウォークや、行列の分解を、高速に行うような話は1つの研究パターンになってきているようだ。
        </UL>
      </UL>
      <LI>チュートリアルでは「Mining for Software Reliability」が良かった。 ソフトウェアの不具合をデータから見つける。
      <UL>
        <LI>関数の呼び出し関係を木で書いて分類してみたり。
        <LI>演者は、ベイズで、1つのうまくいく実行例と、1つの上手くいかない実行例から、モデルをつくる、<A href="http://www-faculty.cs.uiuc.edu/~hanj/pdf/icdm06_chaoliu.pdf">Bayesian debugging</A>というやつで当てたひと。
      </UL>
    </UL>
    <LI>ネットワークものはとりあえず流行っているようだ。
    <UL>
      <LI>ワークショップ（グラフ）と、2セッションと、それなりの数を占めている。
      <UL>
        <LI>ネットワークと銘打ったところ以外のところでも、ネットワーク上という設定の研究はかなり多い。
      </UL>
      <LI>タスクとしては、social networkを対象に、コミュニティや背後の構造の発見。
      <UL>
        <LI>（ネットワーク構造を表す）行列を低ランク近似するテクニックがよく利用される。
      </UL>
      <LI>ネットワークの解析を、時間方向に拡張するというのがひとつのトレンド。
      <LI>ただ、現在は、流行ってる、てことでいいんだけど、今後は、解析の結果が何らかのアクションに結び付いていく、というようにもって行かないとそのうち廃れてしまうのではないかと。
    </UL>
    <LI>個人的にヒットしたのは、「<A href="http://www.cs.carleton.edu/faculty/dmusican/musicantd-AggregateLearning.pdf">Training on aggregated outputs</A>」、問題設定が面白い。
    <UL>
      <LI>訓練集合において、出力が、いくつかのデータを纏めたものに対してのみ与えられるような回帰問題
      <UL>
        <LI>回帰の場合、いくつかの事例の集合に対し、集合内の出力の和が与えられるような設定。
        
        <LI>分類の場合、いくつかの事例の集合にたいし、集合内の出力のクラス分布が与えられるような設定。
      </UL>
      <LI>従来のアルゴリズム（SVM, バックプロパゲーション）の、この問題バージョンを考えた、という話。
      <LI>物理の実験かなんかで、こういう場合があるらしい、というのから一般化したそうだが、目の付け所が面白い。
      <LI>が、よく考えると、結局、従来の場合や既存の手法に帰着されるような気がしてきた...。
      <UL>
        <LI>線形回帰の場合は入力を足したインスタンス作るのと同じっぽい。 まあこれは、目的関数の設計にもよるが。
        <LI>分類の場合は、<A href="http://www.machinelearning.org/proceedings/icml2007/papers/441.pdf">expectation regularization</A>と同じ？
      </UL>
    </UL>
    <LI>今回ためになった話： privacy preserving data mining が何をやっているのか、ちょっと分かった。
    <UL>
      <LI>これまで「（そこで何かができる気はしないから）セキュリティには近寄らない」と思っていたので、意図的に避けてきたのだが、なんとなくワークショップを見に行って、ちょっとだけ気持ちが分かった。
      <LI>扱うのは、データについての情報を漏らさずに、データ解析を行うような話全般
      <OL>
        <LI>ユーザーがサーバーに対してデータを隠しながら所望の解析を行う、とか
        <LI>複数のユーザーがお互いのデータを開示することなく、全データを使った解析を行う、とか
      </OL>
      <LI>基本的には、セキュリティ分野の研究成果である安全なプロトコルを、「部品」として使い、データ解析アルゴリズムを何らかの意味で安全に行う、という方針のように思えた。
      <UL>
        <LI>たとえば、↑の2.の場合には、
        <UL>
          <LI>データの持ち方の仮定によって、2つ設定がある。
          <UL>
            <LI>horizontally-partitioned: 各ユーザーが別々のインスタンスを管理している
            <LI>vertically-partitioned: 各ユーザーが別々の属性を管理している
          </UL>
          つまり、データ行列の情報を、各ユーザーが分担してもっている
          <LI>学習アルゴリズム（logistic regression, k-means clustering, ...）は、行列の計算として書くことができるので、各行列計算の部分をSMC
          (Secure Multi-party protocol) を用いてを実現する。
          <UL>
            <LI>SMCは、行列の足し算や掛け算の持ち寄り計算をコッソリおこなうことができるプロトコル。
            コレ自体は、所与のものとして用いる。
          </UL>
        </UL>
      </UL>
    </UL>
    <LI>会議のあとは、ついでに<A href="http://domino.research.ibm.com/comm/research_projects.nsf/pages/dar.index.html">ワトソン詣で</A>。 伝説の男、<A href="http://www.research.ibm.com/massive/tdl.html">テサウロ氏</A>に会ってみたり。
    <UL>
      <LI>ついてに<A href="http://www.h-eba.com/heba/BG/bg0.html">バックギャモン</A>のルールをちょっと調べてみたり。 <A href="http://www.bkgm.com/motif/">こいつ</A>に全然勝てなかったり。
    </UL>
  </UL>
  <LI><A name="20071012"></A>2007/10/12<A href="http://www.cs.ust.hk/~qyang/ICDMDMC07/"> IEEE ICDM データマイニングコンテスト</A>で <B>優勝 &amp; ３位</B>
  <UL>
    <LI>世界最大のデータマイニング関連国際会議である<A href="http://www.ist.unomaha.edu/icdm2007/">IEEE ICDM</A>で開催された、データマイニングのコンペティション <A href="http://www.cs.ust.hk/~qyang/ICDMDMC07/">ICDM Data Mining Contest</A> で、我々のチームが、片方のタスクでは２位以下に予測精度10％以上の大差をつけて優勝（15チーム中）、もう片方のタスクでは僅差で３位（17チーム中）に入賞しました。
    <UL>
      <LI>結果の表は、<A href="http://www.cs.ust.hk/~qyang/ICDMDMC07/Results.htm">ココ</A>。 我々のチームのIDは５０と５１です。
      <LI>ということで、急遽、ICDMでプレゼンテーションをすることになりました。
      <LI>企業で働く研究者としては、論文以外にも、このような実問題（この場合、既に学習の問題に落としてあるので、半実問題というところだろうけど）において、成果を出せたというのはうれしいところです。
      <UL>
      <LI>一応、看板背負う以上、「出ます」と言った時点で、勝ちを宿命づけられているというプレッシャーも（勝手に）それなりに感じてたりとか...。
      </UL>
    </UL>
    <LI>タスクの内容は、モバイル端末をもったユーザーが、複数アクセスポイントからの信号の強さをもとに自分の位置を推定するというものです。
    <UL>
      <LI>特徴としては、
      <UL>
        <LI>マルチクラス（約２５０クラス）の分類問題 （座標を予測する（＝回帰）のではなく、自分の位置は、クラスラベルで与えられる。アプリケーションのイメージとしては、「いま校長室にいます」とかを当てる感じ？
        ）
        <LI>ラベルのついたデータ数は各クラス１～３個と、とても少ない。 １個のやつとかは、cross
        validationにならない...。
        <LI>半教師つき学習。 ラベルのついてないデータは結構ある。
      </UL>
      <LI>タスク１では、データが時系列で与えられるので、信号だけでなく、時間的な関係を使っても良い（というか、使えと）。
      <LI>タスク２では、訓練データ（昼に採った）とテストデータ（夜に採った）の分布が異なる、いわゆる、covariate
      shift とか、transfer learning とか呼ばれる問題になっている。 <A href="http://sugiyama-www.cs.titech.ac.jp/~sugi/index-jp.html">杉山さん</A>が得意なやつです。
    </UL>
    <LI>前工程の重要性は、やはり今回も痛切に感じた。
    <UL>
      <LI>後工程でがんばるよりも、どんな情報をどう加工して後工程に渡すかというところが結果に大きく効いてくる。 
      <LI>でも、前工程に寄るほどドメイン依存の泥臭い部分が多くなるので、我々の研究対象としては、より抽象的な、後工程のほうに行きがちだし、そっちのほうがオリジナリティを発揮しやすいわけで。
      <LI>今回みたいなコンペティションならとりあえず「勝てばよかろう」なのだけど、研究としては「やるべきところ」というのと「自分だからこそできるところ」をうまくバランスする必要があるだろう。
      
      <LI>そう思ってみると「いろんな種類の情報をどう組み合わせるといいか」という、ちょっと前工程寄りの<A href="http://psb.stanford.edu/psb-online/proceedings/psb04/lanckriet.pdf">情報統合の問題を、カーネルの合成の問題にもっていく話</A>なんてのは、絶妙の立ち位置ですごくうまいなあと思う。
      <UL>
        <LI>ということで、加藤さんが<A href="http://ibis2007.bayesnet.org/program.htm">IBIS</A>で発表する話は、そういう話です、確か。
      </UL>
    </UL>
    <LI>ところで、この手のコンペティションものとしては、データマイニング界最高峰の国際会議<A href="http://www.sigkdd.org/conferences.php">ACM KDD</A>で開催される<A href="http://www.sigkdd.org/kddcup/index.php">KDD cup</A>のほうが長い歴史があります。
    <UL>
      <LI>ちなみに、<A href="http://www.cs.uic.edu/~liub/Netflix-KDD-Cup-2007.html">今年度</A>は、弊社リサーチのワトソン研究所のチームが１位と３位で、我々はコレに触発されたわけで。
      <LI>なお、KDD cupといえば、なんといっても、<A href="http://www.se-se.jp/">瀬々さん</A>。 <A href="http://pages.cs.wisc.edu/~dpage/kddcup2001/">2001年に優勝</A>しています。
    </UL>
  </UL>
  <LI><A name="20070919"></A>2007/09/19 もらってばかりでゴメンナサイ（two papers accepted for <A href="http://nips.cc/Conferences/2007/">NIPS</A>）
    <UL>
    <LI> 仲間にいれていただいた論文が2本、NIPSにアクセプトされました。 めでたい。
    <UL>
    <LI><A href="http://sugiyama-www.cs.titech.ac.jp/~sugi/index-jp.html">杉山さん</A>筆頭の共変量シフトの話
    <LI><A href="http://www.cb.k.u-tokyo.ac.jp/asailab/kato/">加藤さん</A>筆頭のマルチタスク学習の話
    </UL>
    ともに、お二人の得意技が余すところなく炸裂しております。
    <LI> アレ？僕が筆頭のやつはどこ？ 
    </UL>
  <LI><A name="20070905"></A>2007/09/05 <A href="http://sugiyama-www.cs.titech.ac.jp/T-PRIMAL/OpenSeminar/ICML2007/index-jp.html">ICML2007読む会</A>
  <UL>
    <LI>今回も東工大で開催。 最近あまり論文読んでなかったので、最近の研究がまとめてわかってお得。
    知りあいも増えてまたお得。
    <LI>個人的には、加藤さんの紹介した<A href="http://www.machinelearning.org/proceedings/icml2007/papers/229.pdf">コレ</A>がいちばんヒット。 多クラス分類のための次元削減を考えるというもの
    <UL>
      <LI>前提： 各クラスc用ごとにパラメータベクトル<B>w</B><SUB>c</SUB>があって、argmax<SUB>c</SUB> &lt;<B>w</B><SUB>c</SUB>, <B>x</B>&gt; で多クラスの予測を行うような分類器を考える。
      <UL>
        <LI><B>w</B><SUB>c</SUB> と <B>x</B> は、ともにd次元ベクトルとする（＝特徴空間の次元がd）。
      </UL>
      <LI>アイディア： パラメータベクトル<B>w</B><SUB>c</SUB>を並べた行列 W を2つの行列F（d×s次元）とG（s×d次元）の積として、W=FG
      のように分解する。 ただしs&lt;d。 つまり、途中で1回次元数が減っていることになる。
      <LI>問題点と解決： しかし、一般に、行列のランクに制約を入れると、問題が凸でなくなってしまうので、代わりに凸性を保証するtrace
      normを正則化項として使う
      <UL>
        <LI>trace normは固有値を並べたベクトルの1ノルムと等価なので、これを小さくすることは、多くの固有値を0にすることになる。
        つまり、ちょっと遠巻きにランクを最小化してる感じになる。
        <LI>微分は 1ノルムの真ん中を滑らかにしておいてからとる。 毎回、SVDを解く必要があるのでちょっとメンドウ。
      </UL>
      <LI>trace normといえば、<A href="http://www.google.co.jp/search?sourceid=navclient&hl=ja&ie=UTF-8&rls=GGLG,GGLG:2006-04,GGLG:ja&q=large+margin+matrix+factorization">large margin matrix factorization</A>ではじめて見たときにカッコイイなー、何かいい使い道があるかなー、と思ったものだが、コレ、うまく使っている。
      イイ。
    </UL>
    <LI>そのほか、
    <UL>
      <LI>藤巻さんは、マルチインスタンス学習を<A href="http://www.machinelearning.org/proceedings/icml2007/papers/558.pdf">2</A><A href="http://www.machinelearning.org/proceedings/icml2007/papers/409.pdf">本</A>紹介。 その<A href="http://www.machinelearning.org/proceedings/icml2007/papers/409.pdf">片方</A>のやつでは、マルチインスタンス学習を半教師アリ学習として捉える。
      <UL>
        <LI>positive bagに入っている例は、基本的にラベルなしとして扱うんだけど、「うち、どれか一個は必ず正例」という制約をいれる。
        ちょっと特殊な半教師アリ問題として考えることができ、semi-supervised SVMと同じような定式化と最適化のテクニックが使える。
        <LI>最近ちょっと脳内マルチインスタンスブーム。
      </UL>
      <LI>杉山さんは、カーネルトリック（超高次元の内積計算をサボるやつ）ではない、最近浸透しはじめている新しいカーネル法の流れに沿った研究（<A href="http://www.machinelearning.org/proceedings/icml2007/papers/243.pdf">クラスタリング</A>と<A href="http://www.machinelearning.org/proceedings/icml2007/papers/244.pdf">次元削減</A>）を紹介。
      <UL>
        <LI>ガウシアンカーネルとかが、全部のモーメント考えちゃっている的なのが、今ひとつ分かってないが、とりあえず押さえておきたい。
        が、押さえられるだろうか。
      </UL>
      <LI>渡辺さんは、<A href="http://www.machinelearning.org/proceedings/icml2007/papers/472.pdf">構造予測問題に対する乗算タイプの学習法</A>を紹介。
      <UL>
        <LI>dualの作り方がとても不思議な感じでイイ。 確率モデルになっている何か（p）をα（dualのパラメータ）で置き換えて、pとαのKL-divergenceを目的関数に突っ込む→目的関数の（αだけで表現された）上界になって、これを最適化する、という流れ。
      </UL>
    </UL>
    <LI>ちなみに、かしまの読んだのは<A href="http://www.machinelearning.org/proceedings/icml2007/papers/60.pdf">カーネルのsolution path</A>を求める話。
    <UL>
      <LI>ハイパーパラメータを変えたときの解（＝学習済みパラメータ）の軌跡はsolution
      path（最適解パス）とよばれるが、これが効率よく追いかけられると嬉しい（例えば、cross-validationでハイパーパラメータをチューニングするのに、いちいち毎回学習をやり直す必要がないとか）。
      <UL>
        <LI>ハイパーパラメータが正則化項のときには、regularization pathともいう。 
        <LI>たとえば、L1正則化の場合には、多くの（2次までの）損失関数で、solution pathが区分線形になり、効率的に求められる
        <UL>
          <LI>solution pathが、カクっと曲がる点と、曲がったときの向きを計算できればよいので、これらを上手く求めれば、ハイパーパラメータをちょっとづつ変えながら、いちいち全体の最適化問題を解きなおす必要が無い。
        </UL>
        <LI>ちなみに、浜田さんの紹介した<A href="http://www.machinelearning.org/proceedings/icml2007/papers/282.pdf">コレ</A>（津田さんの論文）は、グラフをfeatureにした回帰の問題を、solution pathアルゴリズムとグラフマイニングのアルゴリズムをうまく組み合わせて実現したもの。
        カッコイイ。
      </UL>
      <LI>この論文では、カーネルの中のパラメータ（ガウシアンカーネルの幅とか）を変えたときのsolution pathを求めようというもの。 基本的にはKKT条件の線形方程式の解を追っかける。
      <LI>が、今回の場合、結局一般的にはsolution pathは区分線形になるとはいえないので、普通に探索することになる。
      なので、solution pathのかっこよさがあまり出ていなくて、ちょっと残念な感じだった。
      <LI>パスものというか、「一発でイロイロ解いちゃうよ」的な話は、僕的にグッとくるので、しばらく勉強してみたいところ。
      が、まだ今ひとつピンときてない。
    </UL>
  </UL>
  <LI><A name="20070807"></A>2007/8/8 <A href="http://www.jsbi.org/modules/SIGs_WGs/index.php/summer_school/2007/school2007.html">バイオインフォマティクスのサマースクール</A>で、ネットワーク予測のお話をさせていただきました
  <UL>
    <LI>キノコ屋敷で、3日間にわたって参加してきました。 まさに陸の孤島。
    <LI>個人的に、とても面白かったお話は、富井先生のタンパク質の配列相同性検索の話。
    <UL>
      <LI>２つの配列の類似度を比べるのに、まず、単純な検索方法で、それぞれに似た配列集合を取ってきて、それぞれの集合から、それぞれの確率モデルを作る
      → そして確率モデル同士を比較して、類似度を測る、という考え方。
      <LI>なによりも、実際に、<A href="http://www.aist.go.jp/aist_j/press_release/pr2004/pr20041202_2/pr20041202_2.html">この世界で勝っている</A>のがスゴイ。
    </UL>
    <LI>かしまの話は、タンパク質相互作用ネットワーク予測の話
    <UL>
      <LI>今回は特に、ドメイン情報を用いた手法を中心に（勉強して）お話した。
      <UL>
        <LI>いちおう、問題のとらえかたのひとつを紹介すると、予測したいのは、2つタンパク質が与えられたときに、それらの間にお友達関係があるかどうかを予測したい。
        タンパク質の上には、ドメインという領域がいくつかあって、たとえば「タンパク質iは、ドメインAとドメインCをもっています」という感じで、これをもとに、タンパク質のお友達関係を予測する。
        訓練データは、タンパク質間のお友達／非お友達関係がいくつか与えられている。
        というかんじ。
      </UL>
      <LI>このへんのやり方って、それぞれの事情から出てきたものだが、実は、機械学習的に考えるといろいろ対応があったりする。
      <UL>
        <LI>ドメイン-ドメインの相互作用がひとつでもあれば、タンパク-タンパクの相互作用がある、というのはマルチインスタンス学習（multi-instance
        learning）だったり、
        <LI>「parsimony principle」： 生物はなるべく少ないドメイン間相互作用で、タンパク質間相互作用を作っているはず、というのは（L1）正則化だったり。
        <UL>
          <LI>ところで、質問で（たぶん）生物の方から「生物が、少ないドメイン間相互作用でやろうとしているというのには違和感がある」というご意見を頂いた。
          おそらくこの方の心情は「<U>冗長性がまったくないほどに</U>節約するというのには違和感を覚える」ということだったのだろうと思う。
          
          <LI>ただ、節約を全くしない、というのはあり得なくて、そうでないと、全ドメインが相互作用してしまうという結論になる。
          データを説明するためには、なんらかの意味での正則化、つまり、節約することが必要になるわけだ。
          <UL>
            <LI>正則化の観点からは、別にL1でなくてもL2でもいいわけで（そっちのほうが予測精度がいいかもしれないし）そうなると多少冗長性のあるドメイン間相互作用セレクションをやってくれるのだろう。 
          </UL>
          <LI>ある意味、機械学習のモデルを通して、生物が節約しているというのを裏付けているともいえる（というのは多分言いすぎ）。
          <UL>
            <LI>現在のモデル ＝ ひとつでもドメイン間相互作用があればタンパク質間相互作用がある。
            タンパク質間相互作用に積極的にマイナスに働くドメインは（一般的には）いない、というモデルを信じる限りは。
          </UL>
        </UL>
      </UL>
      <LI>でもって、発表資料は<A href="publication/bioinfosummer2007slide.pdf">ココ</A>におきました。
    </UL>
    <LI>あ、あと、やっぱり僕はまだ、工学と科学の壁を越えていない、そして、越えたがっていないんだと実感しました。
  </UL>
  <LI><A name="20070620"></A>2007/6/20 ガウス過程（Gaussian Process, ガウシアンプロセス）を簡単に説明しよう
  <UL>
    <LI>最近、ガウス過程を使ったものを書いたので、これを、ある方向から簡単に説明することを試みます。
    話をわかりやすくするために、細かいところはウソをつく、というかいい加減に書きますが、とりあえず、僕は「こう理解して差し支えない」と思っています。
    <LI>まず、ガウス過程は、基本的に回帰(regression)をするためのものです。 つまり、入出力の組がN個与えられたときに、N+1個目の入力に対する出力（実数値）を当てたい。
    <UL>
      <LI>つまり、N個の訓練データが与えられたときに、1個のテストデータに対する予測を行いたい。
      <LI>i番目の入出力の組を ( <B>x</B><SUP>(i)</SUP>, y<SUP>(i)</SUP> ) とかくことにします。
    </UL>
    <LI>ガウス過程とは、N個（の訓練データ）の出力 <B>y</B>=(y<SUP>(1)</SUP>, y<SUP>(2)</SUP>, ..., y<SUP>(N)</SUP>) が、N次元の正規分布に従うとしたものです。 <BR>
    <IMG src="GP1.gif" border="0" width="324" height="43">
    <UL>
      <LI>平均<B>0</B>、共分散行列Kの、多次元の正規分布。 共分散行列としては、N個のデータの入力間の適当な類似度行列を使います。
      <LI>イメージ的には、共分散Kの(i,j)成分k<SUB>ij</SUB>が大きいと、yの第i成分と第j成分が近い値をとるようになります。 つまり、「入力が近い＝出力が近い」のイメージ。
      <UL>
        <LI>類似度行列Kは、k<SUB>ij</SUB> = ( <B>x</B><SUP>(i)</SUP>と<B>x</B><SUP>(j)</SUP>の類似度 ) として適当に定義してやればよいです
        <LI>ただし、共分散行列は正定でないといけないので、それを満たす必要があるけど、とりあえず、K<SUB>ij</SUB> &gt; 0 であるように定義してやればオッケー
        <LI>類似度行列として、いわゆるカーネル法にでてくるカーネル行列Kを使うことができます。（正定にならないときには対角成分にちょびっと足す）
      </UL>
    </UL>
    <LI>さて、いま、テストデータに対する出力（N+1個目の出力 y<SUP>(N+1）</SUP>）を予測したいとします。 今度は、N+1個のデータ（訓練＋テストデータ）の出力がN+1次元の正規分布に従うとします。
    いま知りたいのは、N個の訓練データの出力が分かっているときの、N+1個目のデータの出力です。
    その確率分布は条件付の正規分布の式を使えば以下のようにかけます。<BR>
    <IMG src="GP2.gif" border="0" width="276" height="33"><BR>
    つまり、予測値は、その期待値である <B>k</B>K<SUP>-1</SUP><B>y</B><SUP>T</SUP> となります。
    <UL>
      <LI>ここで、<B>k</B> = ( k<SUB>1,N+1</SUB>, k<SUB>2,N+1</SUB>, ..., k<SUB>N,N+1</SUB> ) は、テストデータと、これまでのN個の訓練データとの間の類似度をならべたベクトル
      <LI>共分散Σも適当に決まっているのですが、予測とは関係ないので、ココでは気にしなくてよいです。
    </UL>
    <LI>結局、計算するべきものとしては、
    <UL>
      <LI>N個の訓練データの類似度行列の逆行列
      <UL>
        <LI>ガウス過程には、「類似度行列のパラメータを自動チューニングする」などということをやらなければ、いわゆる「学習フェーズ」的なものは存在しません。
        ただ、テスト前に計算しておけるものとしては、この逆行列。
      </UL>
      <LI>テストデータと訓練データの間の類似度（ベクトル）
      <UL>
        <LI>こっちはテストデータが与えられたときに計算します。
      </UL>
    </UL>
    <LI>ポイントは、
    <UL>
      <LI>全てのデータの出力の同時分布が、多次元の正規分布で定義されること。
      <LI>出力間の関係を決める共分散行列として、入力間の類似度行列（カーネル行列）をつかうこと。
    </UL>
  </UL>
  <LI><A name="20070613"></A>2007/6/13 マルチタスク学習での属性選択
  <UL>
    <LI>マルチタスク学習では、複数の類似した（通常教師アリ）学習タスクを同時に解くような問題。
    各タスクにおける事例の数は多くないが、もしタスク同士が似ているのならば、それらのモデル同士も似ているはずだから、モデルの学習においても何らかの形で情報を使いまわせるだろう、というものです。
    <LI>通常、これを実現するのに「タスク同士ではパラメータが似ている」という感じの制約を（事前分布ないし正則化項として）入れて、全タスクの学習問題を一度に求めるような最適化問題を解きます。
    <LI>ところで、マルチタスク学習における特徴選択がどうあるべきかを考えたのが<A href="http://books.nips.cc/papers/files/nips19/NIPS2006_0251.pdf">この論文</A>。
    <LI>その心は、「なるべく共通の特徴を複数のタスクで使うようにしよう。 共通で使われる特徴を選ぼう。」
    <LI>この心を込めたのが、(1,2)ノルムというやつ。 これを使うと、どのタスクでも使われないような特徴が増える。
    <UL>
      <LI>パラメータ行列Wの、各列が各タスクのパラメータベクトルとする。（つまり各行はあるひとつの特徴に対応）
      <LI>(1,2)-ノルムとは、「各行の2ノルムをとってできたベクトルの1ノルム」
      <LI>1-ノルムは、特徴がスパースになる、つまり、使われる特徴の数を少なくする効果があるというのが定説であるが、これによって、各行、つまり各特徴の、タスクをまたいだ2ノルムを抑える。
    </UL>
    <LI>よって、タスクをまたいで、スパースな特徴選択ができるのだと。
    <LI>この論文では、さらに特徴生成までやるんだけど、それは僕のレベルを超えているため割愛。 すまん。
  </UL>
  <LI><A name="20070531"></A>2007/5/31  成仏とか、Rとか
  <UL>
    <LI><A href="publication/Risk_IEICE.pdf">電子情報通信学会に出していたジャーナル</A>が受理された。
    <UL>
      <LI>cost-sensitive learningでconditional value-at-riskを目的関数にするやつ。
    結果的にショートペーパーとジャーナルと、あまり回収できなかった。 やったときには、相当気に入ってたんだけどなー。
    何かをやったよ、とおもったんだけどなー。
    </UL>
    <LI>Rがいい感じ
    <UL>
      <LI>Rが自分の回りでにわかに流行っている。 学習ものを、MATLABなしという制約の中で、お手軽に実装できる最良の選択だと思った。
      しばらくコレでいこうと思う。
      <LI>１からn-1まで数えたいときには、 1:n-1 ではなく 1:(n-1) 、これでハマった…。
    </UL>
    <LI>近頃、同世代のスゴイ人たちと交わらせていただく機会がいろいろとあるのだが、そういう人たちをみていると、テクニカルなところ、スピード、馬力、とにかく全然かなわん、と思う。
    そういうスゴイ人たちが、アイデアから、論文を書くまでの過程を横からみるというのは、とても勉強になる。
    一方、そういうスゴイ彼らと、協力していくにしても、戦っていくにしても、その中に単純にのまれていくのではなく、その中での自分の差分を効果的に押し出していくことが必要だと思うこのごろ。
  </UL>
  <LI><A name="20070525"></A>2007/5/25 バイオインフォのサマースクールでネットワーク構造予測の話をします
  <UL>
    <LI>幹事の渋谷さんに、声をかけていただき、8月の<A href="http://www.jsbi.org/modules/SIGs_WGs/index.php/summer_school/2007/school2007.html">バイオインフォマティクスのサマースクール</A>でお話をさせていただくことに。
    <LI>ネットワークの構造予測のもろもろ話をする予定ですが、僕はちゃんとバイオできてないので、バイオ＆インフォでやりたいひとに有用な話ができるかは不明。
    <LI>が、そのほかの先生方はちゃんとバイオもインフォもある方々です。個人的にも、その話聴くのが楽しみかも。
  </UL>
  <LI><A name="20070505"></A>2007/5/5 <A href="publication/JSAI_network_review.pdf">ネットワーク予測のチュートリアル</A>を人工知能学会誌に書きました
  <UL>
    <LI>ネットワークの構造予測についてまとめたものを、今月の人工知能学会誌のベイジアンネットワーク特集の一記事として書きました。
    <UL>
      <LI>ベイジアンネットに関係ないじゃん、と思われたら、それは、僕が企画の趣旨をちゃんと理解できていなかったせいです。
      ごめんなさい。
    </UL>
  </UL>
  <LI><A name="20070405"></A>2007/4/5 いろいろいただきもの、ありがとうございます
  <UL>
    <LI>学位をいただきました、ありがとうございます。
    <UL>
      <LI>その過程は<A href="http://www.geocities.jp/kashi_pong/research_diary_for_Dr.html">ココ</A>にまとめました、ありがとうございます。
    </UL>
    <LI>人工知能学会の<A href="http://www.ai-gakkai.or.jp/jsai/info/award-paper.html#PAPER">論文賞</A>をいただけるもよう、ありがとうございます。
    <UL>
      <LI><A href="http://www.donald.ai.kyutech.ac.jp/~hiroshi/">坂本先生</A>と、小柳くんと一緒にだした<A href="http://www.geocities.jp/kashi_pong/publication/treeKernel_JSAI.pdf">木カーネルの論文</A>が、ありがとうございます。
      <UL>
        <LI>いやもうホント、坂本先生にいただいた証明ですよ、ありがとうございます。
      </UL>
    </UL>
  </UL>
  <LI><A name="20070226"></A>2007/2/26 DPとネットワーク予測 <FONT color="#ff0000">(ウソを含んでいたので修正しました)</FONT>
  <UL>
    <LI><A href="#20070218">まえの話</A>で、メンバーの多いグループが新しいメンバーを獲得しやすくなるとかいうのがありましたが、これって、なんかスケールフリーネットワークの生成モデルとして有名な優先的選択(Preferential
    Attachment)モデルみたいですね。
    <LI>スケールフリーネットワークの性質のひとつとして、ノードの次数分布がベキ分布に従うというのがありましたが、同様に、DP（ディリクレ過程）ではメンバー数の分布がベキに従います。
    <UL>
      <LI>最近では、ピットマン-ヨー過程(Pitman-Yor process)なんてものもあるようですが、これはパラメータが一個多いDPみたいなもんで、DPによって生成されるメンバー分布のベキの肩の数が固定であるのに対し、パラメータによって、この肩の数をある程度コントロールすることができます。
    </UL>
    <LI>ところで、ディリクレ過程とベキ則が繋がっている、とかいうと、そこから想像して、じゃあ、ディリクレ過程を使って、ネットワークの構造予測ができるんじゃないかと思ったりします。
    <LI>それをやっているのが、<A href="http://scholar.google.com/scholar?hl=ja&lr=&q=%22Learning+Systems+of+Concepts+with+an+Infinite+Relational+Model%22&lr=">NTTの上田さんや山田さんが著者に入っているAAAIの論文</A>(これらの論文は栗原さんに教えてもらいました)。 この論文自体は、ネットワークの構造予測を目指したものではなく、クラスタリングですが、（また、ネットワーク構造よりももっと一般的な関係学習の文脈でですが、）同じモデルはネットワーク予測にも使うことができます。（ちなみに別のドイツチームも、<A href="http://scholar.google.com/scholar?hl=ja&lr=&q=%22Infinite+Hidden+Relational+Models%22&lr=">同じモデル</A>を提案してたりします。が、分かりにくい。）
    <UL>
      <LI>彼らのモデルでは、まず、DPによって、各ノードが、それぞれある状態を与えられます。ここまでは、前回のDPのデータ＝ノードだと思ったのと一緒。
      <LI>次に、2つのノードの間にリンクができるかどうかが、2つのノードの属するグループによって決まります。
      <UL>
        <LI>つまり、２つのノードが属するグループのペア（A,B）に依存して、ある確率p(A,B)で、これらの間にリンクが生成される確率が決まるとします。
        生成の過程では、これは事前分布のβ分布によって決まります。
      </UL>
      <LI>まとめると、
      <UL>
        <LI>ノードごとの属するグループ ～ CRP
        <LI>２つのグループのそれぞれのノード同士がリンクする確率 ～ Beta
        <LI>あるノード同士がリンクする確率 ～ Bernoulli
      </UL>
      のような仕組みでネットワーク構造が生成されるのです。 うーん、爽やかだなあ。
    </UL>
  </UL>
  <LI><A name="20070218"></A>2007/2/18 ディリクレ過程（Dirichlet process）、あるいは、Chinese Restaurant
  Process （CRP）
  <UL>
    <LI>どうも流行っているように見えるディリクレ過程、あるいは、Chinese Restaurant Process （CRP）、なんか難しそうなんだけど、最近、栗原さんやらに色々訊いて、なんとなくイメージがわいてきた。
    <UL>
      <LI>ディリクレ過程と言うべきか、CRPと言うべきかの違いは、最終的な状態を生成する確率分布を意識するとディリクレ過程で、こういうサンプリングの仕方、というのを意識するとCRPなんでしょうか、たぶん。
    </UL>
    <LI>で、結局、（僕くらいの人に関係ある程度で）何かというと、<BR>
    <I>N個の「データ」のそれぞれが、M個の「グループ」のどれかに属している状態
    </I>（下図参照）<BR>
    をつくりだすための機械だということ。<BR>
    <IMG src="DP.jpg" border="0" width="261" height="203" alt="ディリクレ過程の説明">
    <UL>
      <LI>どういう状況でそんなものが必要になるかというと、N個のデータをM個のグループに分割する「クラスタリング」なんてのがまさにそう。
    </UL>
    <LI>じゃ、別に、いままでクラスタリングの方法とかいろいろあったわけだし、なんでそんなものが今もてはやされているのかというと、理由があって、ざっくりいうと、
    <UL>
      <LI>特徴 １） グループの数 M をあらかじめ決めておく必要がない。 なんなら無限個でもいい。
      <UL>
        <LI>（クラスタリングのクラスターの数を決める、ってのは長きに渡って自明でない話だもんね）
      </UL>
      <LI>特徴 ２） 計算が簡単
      <UL>
        <LI>（特徴１を実現しようと思ったときに考えられるであろう他の方法よりも）
      </UL>
    </UL>
    という特徴があるからなのです。
    <LI>では、まず特徴１を実現するために、CRPがどういう風に、<I>N個の「データ」のそれぞれが、（予め固定されていない）M個の「グループ」のどれかに属している状態</I> をつくるのかという手続きを見てみると、
    <OL>
      <LI>「データ」も「グループ」もなにもない状況から、データをひとつづつ加えていく状況を想像しましょう。
      <LI>最初のデータは、まだグループがひとつもない状態なので、自分だけの新しいグループをつくって、そこに属します。
      <LI>2個目以降のデータは、
      <UL>
        <LI>ある小さい確率で、新しいグループを作って、そこに属します。
        <LI>そうでない場合、現在存在するグループのどれかに加わるわけですが、このとき、各グループの現在のメンバー数に比例した確率で、属するグループを決めます。
      つまり、たくさんメンバーのいるグループに加入しやすくなります。
      </UL>
    </OL>
    本当は、ステップ３の記述が不正確ですが、まあ、大体、こんなもんです。 グループ数が予め固定されているわけではなく、データが追加されるごとに、グループが増える可能性がある点がポイントです。
    <LI>さて、特徴２が達成されているかについて考えてみると、
    <UL>
      <LI>確かに↑で見たCRPのサンプリング方法は簡単ですが、このやりかたで、どんどんいろんな状態をサンプリングしようとすると、1つの状態をつくるのに、いちからサンプリングしなおさないといけない（サイコロをN回振らないといけない）ので、大変です。
      最初のひとつは↑に従って作るにしても、2つ目からは、いまあるある状態から、別の状態をサイコロ1回で作り出せたら便利そうです。
      <LI>実は、CRPでは、コレが可能で、
      <OL>
        <LI>現在の状態から、どれかデータをひとつ取り除く （N-1個のデータがある状態になる）
        <LI>その取り除いたデータを、さきほどのサンプリング法で、どれかのグループに再配置する。（あるいは、自分だけのあたらしいグループを作る。）
        <UL>
          <LI>取り除いたデータが、あたかも最後に到着したデータかのように扱われる。
        </UL>
      </OL>
      というようすると、↑のCRPの手続きを全部やり直す手間かける必要が無くなるのです。
    </UL>
    <LI>従って、特徴１と特徴２によって、「N個のデータのそれぞれが、M個のグループのどれかに属している状態」というのをじゃんじゃん生成できるという目標が達成できるのです。
    <LI>で、このCRP、どうやって学習に使うかというと、基本的には、事前分布として（あるいは正則化項として）使うイメージ。
    <UL>
      <LI>↑でみたように、基本的にはサンプリングでやることになっちゃうけど、特徴２のおかげで、事後分布をGibbsサンプリングとかで割と簡単に実現できると。
    </UL>
  </UL>
  <LI><A name="20070211"></A>2007/2/11 <A href="http://sugiyama-www.cs.titech.ac.jp/T-PRIMAL/2007/NIPS2006seminar/index-jp.html">NIPS読む会</A>
  <UL>
    <LI>もうだいぶ前になっちゃったけど、1/26に、東工大（大岡山）で開催されたNIPS読む会に行ってきた。
    <LI>前半はNIPSで発表したひとたちの講演。 杉山さんの共変量シフトの話は、今回はmixtureの混ざり度が変化するというもの。
    今回は<A href="http://ida.first.fraunhofer.de/projects/different06/">共変量がらみのワークショップ</A>まで開催したらしい。 スゴイ。
    <UL>
      <LI>ついでに紹介されていた2つの分布が異なるかどうかの指標が面白かった。 理論はよくわからないが、つまるところ、
      <UL>
        <LI><A href="http://www.kyb.mpg.de/publications/attachments/mmd_final_4193%5B1%5D.pdf">A Kernel Method for the Two-Sample-Problem</A>
        <UL>
          <LI>「2つの分布から出たサンプルの集合にどんな変換（連続の関数）を加えても、平均が等しいなら、ふたつの分布は等しい」というのがあって、ここから結局「2つのデータセットの（RKHSでの）平均のユークリッド距離で2つの分布の距離を測れる」という話になるらしい。
        </UL>
        <LI><A href="http://www.kyb.mpg.de/publications/attachments/Huang_915_4194%5B0%5D.pdf">Correcting Sample Selection Bias by Unlabeled Data</A>
        <UL>
          <LI>↑のやつを共変量シフトの話に応用。 訓練データの平均を（RKHSで）線型変換したのが、テストデータの平均になるような変換を求める。
          で、この変換を使って訓練データの分布を補正すればいいじゃんみたいな話。
          <LI>従来、シフト前とシフト後の分布を推定してその比を使って補正だったのが、比を直接推定する問題としてうまく定式化できた。
          スゴイ。
        </UL>
      </UL>
    </UL>
    <LI>午後は各々が面白そうな論文紹介。 とくに面白かったのが、<A href="http://ai.stanford.edu/~galel/papers/MaxMargin.pdf">Max-margin 
classification of incomplete data</A>
    <UL>
      <LI>属性が欠けているデータがある場合のSVM。 属性が欠けているデータについては、その属性の次元を使わずにマージンを定義する（観測されている部分空間でマージンを定義する）。
      <LI>問題は凸じゃないけど、「例ごとに変な重みのついたSVMを解く（これは凸）」のと、「解いたSVMからその重みを求める」のを繰り返すやりかたで、いい感じにそれなりに解く。
      <LI>ところで、そもそもprimalで属性が欠けるとかの話なので、カーネルは使えない。
    </UL>
  </UL>
  <LI><A name="20070124"></A>2007/1/24 <A href="http://www.comp.hkbu.edu.hk/~wii06/icdm/">IEEE ICDM2006</A>
  <UL>
    <LI>おくればせながら、昨年末に行ってきたICDM （<A href="http://www.data-mining-forum.de/index.php">IEEEじゃないICDM</A>もあったりする）。
    <LI>ICDMは一応データマイニングのトップノッチ会議のひとつ。
    <UL>
      <LI>ということらしい。（<A href="http://www.cs.albany.edu/~ashwin/Conf_rank.html">ココ</A>とか<A href="http://www.cs.ualberta.ca/~zaiane/htmldocs/ConfRanking.html">ココ</A>とか<A href="http://www.infotech.monash.edu.au/research/internal-info/ranking-list/artificial-intelligence-and-related-subjects-280200.htm">ココ</A>とかで、誰が決めたのか分からないランキングがみれます。）
      <LI>ACMのほうの<A href="http://www.acm.org/sigs/sigkdd/">KDD</A>に対抗意識を燃やしているということで、とにかく、いろいろとKDDよりも多いのがじまん。
      （<A href="http://www.cs.uvm.edu/%7Eicdm/ICDMAcceptanceRates.shtml">表</A>とか<A href="http://www.cs.uvm.edu/%7Eicdm/ICDM-KDDNumbers.shtml">グラフ</A>とか）
    </UL>
    <LI>今回は、<A href="http://www.comp.hkbu.edu.hk/~wii06/wi/?index=about">WI(Web Intelligence)</A>/<A href="http://www.comp.hkbu.edu.hk/~wii06/iat/?index=about">IAT(Intelligent Agent Technologies)</A>との3会議共同開催、人間多し。
    <LI>会場は、Hong Kong Convention &amp; Exihibition Centre。 近代的な建物で、<A href="http://giw.ims.u-tokyo.ac.jp/giw/index.html">GIW</A>やってた恵比寿のなんとかセンターとか横浜のなんとかセンターみたいな感じ。
    <UL>
      <LI>高そうな会場とか、バンケットが船だったりとか、そんなこんなで<A href="http://www.comp.hkbu.edu.hk/~wii06/icdm/?index=registration">参加費900＄くらい</A>。
    </UL>
    <LI>パネル「データマイニングアルゴリズム TOP10」などというカウントダウンTVばりの面白企画まで
    <UL>
      <LI>データマイニングの10大アルゴリズムを投票で選ぼう （<A href="http://www.cs.uvm.edu/~icdm/algorithms/ICDM06-Panel.pdf">プレゼンテーションスライド</A>）
      <UL>
        <LI><A href="http://www.cs.uvm.edu/~icdm/algorithms/CandidateList.shtml">事前にノミネートされたアルゴリズム（18個）</A>にコミッティメンバーが投票。 会場でも票をとる。
        <UL>
          <LI>18個のなかに、gSpanが入ってた。 イヤ、それならここにはかわりに「グラフマイニングを創った」AGMが入るべきじゃないかなーと思ってみたり。
          この辺はもう声のでかさというか、社交を通じて過去の自分の研究を育てるっていうのが大切だあなと。
        </UL>
      </UL>
      <LI>トップ３は
      <OL>
        <LI>C4.5 (Quinlan)
        <LI>SVM (Vapnik)
        <LI>Apriori (Agrawal)
      </OL>
      5番目に入っているEMはなんか次元が違う気がするけど、まあいいか。
      <LI>ちなみに昨年のパネルは「<A href="http://www.cs.uvm.edu/~icdm/10Problems/index.shtml">10 challenging problems</A>」だったようだ。
    </UL>
    <LI><A href="http://www.comp.hkbu.edu.hk/iwi06/icdm/?index=award">ベストペーパー</A>は 「Fast Random Walk with Restarts」。
    <UL>
      <LI>内容は、PageRankのような、ランダムウォークによって、グラフのノードの重要度や類似度を計算する方法を速くするというもの。
      <LI>大きな行列の逆行列、あるいは、掛け算を繰り返し行う必要がある。 もともとの行列のスパース性を利用して、「ブロック対角化＋余りを低ランク近似」すると逆行列計算が速い。
      <LI>獲ったのはファローソス氏の弟子、この辺の人はいつも何か獲っている気がする。
      <UL>
        <LI>ちなみに氏は、別にもなにかの賞（貢献賞みたいなやつ）をもらってた関係でスピーチ
        <UL>
          <LI>注目する分野は、
          <UL>
            <LI>バイオ
            <LI>グラフ、ソーシャルネットワーク
            <LI>ストリーム／時系列
            <UL>
              <LI>特に、ストリーム／時系列に関連してグラフの時系列について言及 － 井手さんの時代到来
            </UL>
          </UL>
          <LI>アクセプタンスの多い論文
          <UL>
            <LI>協調フィルタリング、パーソナライズ
            <LI>確率モデルとかのラージスケール化
            <LI>ストリーム
          </UL>
        </UL>
      </UL>
    </UL>
    <LI>発表いくつか <FONT size="-2">（じつはあまり真面目に聴かなかった）</FONT>
    <UL>
      <LI>時系列の決定木 “decision trees for functional variables”
      <UL>
        <LI>時系列の分類問題をとく。
        <LI>枝分かれの点では、２つの時系列のどっちに近いかで決める。
        <LI>その２つの時系列は、クラスタリングでつくる。
      </UL>
      <LI>グラフストリーム “Pattern Mining in Frequent Dynamic Subgraphs”
      <UL>
        <LI>グラフが時系列でやってくる場合に、頻出パタンを拡張。
        <LI>ノードの集合は固定、枝の有無が変化する。
        <LI>エンロンのメールデータを使った。
        <LI>ちなみに彼は、グラフカーネルまわりをいろいろとやってくれているひとで、今回実物に遭遇。
      「father of graph kernel」といわれご満悦。
      </UL>
      <LI>アソシエーションルールで分類 “lazy associative classification”
      <UL>
        <LI>分類のためのアソシエーションルール ＝ 左辺がパタン、右辺はクラス。
        <LI>decision treeの一般化： decision tree は greedy につくるけどこれは最適なやつを見つける。
        <LI>従来法の問題点： テストケースをカバーしない場合がある。
        <LI>アイデア： テストデータをみるたびにやろう。
        <UL>
          <LI>使われたfeatureにのみ射影してルールをほる。
          <LI>共通にでてくるルールはキャッシュする。
        </UL>
      </UL>
      <LI>マニフォルド正則化のソルーションパス “solution path for manifold regularization”
      <UL>
        <LI>正則化パラメータなどのハイパーパラメータを動かしたときの解（＝モデル）の軌跡を得る
        <LI>コレがすごいというより、もともとのソルーションパス自体を知らなかったので面白い。
        <UL>
          <LI>ちなみに、ソルーションパスでいろいろと成果を出しているのは、またあのサハロンだったりする…。
        </UL>
      </UL>
    </UL>
    <LI>感想： ICDM（を含むデータマイニングのいい会議）はそこに居ることに意味がある
    <UL>
      <LI>多くの発表において、体系のどこかに位置づけられるというより、とにかく、新しそうな問題を見つけてきて、がんばってイロイロ考えてなんとかやりました、みたいな印象がある。
      <UL>
        <LI>悪く言えば、根無し草的な手法がおおいなー的な。
        <LI>これまでになかったあたらしい問題をみつけて取り組んでいるといえば、そうなのだが、本当にそうなのかというと、その検証はあまりない気がする。
      </UL>
      <LI>従って、自分が解く必要に迫られていない問題のヒューリスティクスを聞いてきたかんじで、何かを学んだ気があまりしない。
      <LI>でも、やっぱり一流の人たちも集まるし、いい会議であるのは間違いない。 そこで学ぶことに価値があるというよりも、そこに居ることに価値がある。
      <UL>
        <LI>そこで発表されたものを踏み台に「自分が」新しい研究をする可能性があるというよりは、自慢する場というか万博的雰囲気というか。
      </UL>
    </UL>
  </UL>
  <LI><A name="20070122"></A>2007/1/22 とりあえず近況
  <UL>
    <LI>相変わらず、EMっぽいやつが、ちゃんとEMになっているかがわからない。
    <LI>とりあえず<A href="http://www23.atwiki.jp/ilcorgi-n-corgi/">NIPS読む会</A>
    <UL>
      <LI><A href="http://www15.atwiki.jp/tsuboi/pages/4.html">ICML読む会</A>に引き続き。 今回は「<A href="http://books.nips.cc/papers/files/nips19/NIPS2006_0691.pdf">速く解けるMDPのクラス</A>」をよむ。
      <UL>
        <LI>マルコフ決定過程とは、状態i から状態j への遷移確率がアクションaに依存して決まるような「環境」と、各状態を訪れたときに発生する「コスト」が与えられたときに、「将来にわたって発生するコストが最小になるようなアクション」を決定するような問題。
        <UL>
          <LI>通常、value iterationとかpolicy iterationとかよばれる方法によって、「状態の評価値」と「各状態でとるべきアクション」を交互に推定するようなやりかたになる。
        </UL>
        <LI>この論文では、状態i から状態j への遷移確率が p<SUB>ij</SUB> = <U>p</U><SUB>ij</SUB> exp(u<SUB>ij</SUB>) のようにして、コントロールできない確率<U>p</U><SUB>ij</SUB> と、連続値アクションu<SUB>ij</SUB>によって決まるようなMDPを考えると、value iterationとかpolicy iterationとかナシで、固有値問題一発で解けるもよう。
        なんかカッコイイぞ。
        <UL>
          <LI>shortest pathも解けたりする。
          <LI>普通の離散入力のMDPも、コレで近似できたりする。
        </UL>
      </UL>
    </UL>
    <LI>とりあえず<A href="http://www.google.com/analytics/ja-JP/">GoogleAnalytics</A>いれてみた。
  </UL>
  <HR>
  <P>ちなみに、このサイトの掲載内容は私自身の見解であり、必ずしもIBMの立場、戦略、意見を代表するものではありません</P>
  <script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1197929-1";
urchinTracker();
</script>
</UL>
</BODY>
</HTML>
