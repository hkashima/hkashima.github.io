<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Statistical Learning Theory (2024), Graduate School of Informatics, Kyoto University</title>
<meta http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
<meta content="MSHTML 6.00.6000.17093" name="GENERATOR">
<meta http-equiv="Content-Style-Type" content="text/css"></head>
<body>
<p><font size="+2"><b>Statistical Learning Theory (2024), Graduate School of Informatics, Kyoto University<br>
</b>
</font><br>
Lecturers: <a href="https://hkashima.github.io/index.html">Hisashi Kashima</a>, <a href="https://riken-yamada.github.io/profile.html">Makoto Yamada</a>, and <a href="https://koh-t.github.io/jindex.html">Koh Takeuchi</a><br>
  Day, time, and room: Monday, 8:45-10:15 / Research bldg. 8, Lecture room 2<br>  
<br>
<br>

This course will cover in a broad sense the fundamental theoretical aspects and applicative possibilities of statistical machine learning, which is now a fundamental block of statistical data analysis and data mining. This course will focus on the supervised and unsupervised learning problems, including theoretical foundations such as a survey of probably approximately correct learning as well as their Bayesian perspectives and other learning theory frameworks. Several probabilistic models and prediction algorithms, such as the logistic regression, perceptron, and support vector machine will be introduced.
Advanced topic such as online learning, transfer learning, and sparse modeling will be also introduced.

<br>
<br>
<strong>[Course Materials]</strong> <br>
 1. <a href="SML2024_introduction.pdf">Introduction to Machine Learning</a> (Kashima)<br>
 2. <a href="SML2024_regression.pdf">Regression</a> (Kashima)<br>
 3. <a href="SML2024_classification.pdf">Classification</a> (Kashima)<br>
 4. <a href="https://groups.oist.jp/mlds/statistical-learning-theory">Feature Selection and Sparsity</a> (Yamada)<br>
 5. <a href="https://groups.oist.jp/mlds/statistical-learning-theory">Dimensionality Reduction</a> (Yamada)<br>
 6. <a href="SML2024_learningtheory.pdf">Statistical Learning Theory</a> (Kashima)<br>
 7. <a href="SML2024_neuralnet.pdf">Neural Networks</a> (Kashima)<br>
<br>


<strong>[References]</strong><br>
- Lecture slides for the previous years: 
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2014.html">2014</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2015.html">2015</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2016.html">2016</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2017.html">2017</a>, 
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2018.html">2018</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2019.html">2019</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2020.html">2020</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2021.html">2021</a>,
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2022.html">2022</a>
  <a href="https://hkashima.github.io/course_statistical_learning_theory_2023.html">2023</a>
  <br>
<br>


</body></html> 
